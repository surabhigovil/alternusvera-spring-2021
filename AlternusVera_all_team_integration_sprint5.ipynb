{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlternusVera_all_team_integration_sprint5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fLx1PK9oogYU",
        "nUfbWvK2OBZ5"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ozZMQ-jqp0j"
      },
      "source": [
        "# CMPE 257 - MLSprings 2021 Cohort\n",
        "## Objective: Detect fake news in political datasets using factors\n",
        "\n",
        "## Factors by Team\n",
        "## Team Equality - Abhishek Bais, Haley Feng, Jimmy Liang, Shannon Phu\n",
        "### Abhishek - Misleading Intentions\n",
        "#### Microfactors:  \n",
        "Sentiment Analysis  \n",
        "Sensationalism  \n",
        "Click Bait \n",
        "\n",
        "Datasets:\n",
        "1. [Politifact](drive.google.com/file/d/1LUTnGJ1c8WDwcEmed85GhfoEUm2sJFzN/view)  \n",
        "2. [Amalgamated dataset from varied newsAPI feed](https://docs.google.com/spreadsheets/d/1jJflezhjlTPRoVHvj7UvQwllssq66zhZNPz7GI_Zwhc/edit#gid=22382224)  \n",
        "3. [Sensational words corpus](https://drive.google.com/file/d/1JIes9QhZw7EUt59EBDUgrdFMokoT1W8u/view)  \n",
        "\n",
        "### Shannon - Stance Detection\n",
        "#### Microfactors:  \n",
        "Sentiment Analysis  \n",
        "Subjectivity Score  \n",
        "BERT Embeddings  \n",
        "\n",
        "Datasets:\n",
        "1. [Fake News Challenge](https://www.kaggle.com/c/fakenewskdd2020)  \n",
        "2. [Politifact](drive.google.com/file/d/1LUTnGJ1c8WDwcEmed85GhfoEUm2sJFzN/view)  \n",
        "3. [Sentiment words corpus](https://drive.google.com/file/d/1JIes9QhZw7EUt59EBDUgrdFMokoT1W8u/view)  \n",
        "\n",
        "### Haley - Political Bias\n",
        "#### Microfactors: \n",
        "Sentiment Analysis  \n",
        "Party Affiliation  \n",
        "Vocab Selection Bias  \n",
        "1. Politifact\n",
        "2. [GoogleNews API](https://docs.google.com/spreadsheets/d/1Uu-266Q0ab88fnnjtrZ8MMMV8KGNzGoiGKXbBssza2s/edit?usp=sharing)\n",
        "3. [Ideological Book Corpus](https://people.cs.umass.edu/~miyyer/ibc/index.html) / [Kaggle Tweets](https://docs.google.com/spreadsheets/d/14KRtIdMqbp1Tnd7AraR-ROtPDTSQgc--hMmm0L7baDc/edit?usp=sharing)\n",
        "\n",
        "### Jimmy - Naive Realism\n",
        "#### Microfactors:  \n",
        "Topic Centrality  \n",
        "Polarization  \n",
        "Source Centrality  \n",
        "\n",
        "## Team DataCorps - Yuxing Wang, Arun Talkad, Mayuri Lalwani\n",
        "\n",
        "### Yuxing - Psychology Utilities\n",
        "#### Microfactors:\n",
        "Group confirmation<br />\n",
        "Opinion leader<br />\n",
        "Sentiment<br />\n",
        "Datasets:\n",
        "1. Politifact\n",
        "2. Twitter API\n",
        "3. News API\n",
        "\n",
        "### Mayuri - Intent\n",
        "#### Microfactors:\n",
        "Utterance<br />\n",
        "Speech<br />\n",
        "Sentiment<br />\n",
        "Datasets:\n",
        "1. politifact\n",
        "2. twitter\n",
        "3. newsapi\n",
        "\n",
        "### Arun - Source Reputation, Source Reliability\n",
        "#### Microfactors:\n",
        "Provenance Analysis <br />\n",
        "News Subjectivity   <br />             \n",
        "News Credibility  <br />              \n",
        "News Veracity Detection <br />\n",
        "Datasets:\n",
        "1. Politifact\n",
        "2. Twitter API\n",
        "3. News API <br />\n",
        "\n",
        "## Team Sparrow \n",
        "### Princy\n",
        "#### Microfactors\n",
        "Text similarity<br/>\n",
        "Sentiment Polarity <br/>\n",
        "Datasets:\n",
        "1. [Stance Dataset](http://www.fakenewschallenge.org)\n",
        "2. [ISOT Fake News Dataset\n",
        "](https://www.uvic.ca/engineering/ece/isot/datasets/fake-news/index.php)\n",
        "3. [Kaggle](https://www.kaggle.com/c/fake-news/)\n",
        "\n",
        "## Team Amalgam\n",
        "### Surabhi: Credibility\n",
        "#### Microfactors\n",
        "Author Experise<br />\n",
        "Content Credibility<br />\n",
        "Text Readability<br />\n",
        "Datasets Used: \n",
        "1. Scraped Data from Politifact website \n",
        "2. Scraped news article from web\n",
        "\n",
        "### Arpitha:  Style based approaches\n",
        "#### Microfactors\n",
        "Hyperpartisan<br />\n",
        "Yellow Journalism<br />\n",
        "Deception/Lying in text<br />\n",
        "Datasets Used: \n",
        "1. Kaggle fake news dataset: https://www.kaggle.com/surekharamireddy/fake-news-detection\n",
        "2. SemEval Hyperpartisan News Detection task dataset: https://pan.webis.de/semeval19/semeval19-web/\n",
        "\n",
        "### Gayathri: Authenticity\n",
        "#### Microfactors\n",
        "Flesch Reading Ease Score<br />\n",
        "Polarity score<br />\n",
        "Subjectivity Score<br />\n",
        "Datasets Used: \n",
        "1. Scraped Data from Politifact website\n",
        "\n",
        "\n",
        "\n",
        "## Team Underdog \n",
        "### Jocelyn \n",
        "### Source Reputation\n",
        "#### Microfactors\n",
        "Source Ratings Score \n",
        "Reputation Score \n",
        "Sentence Similarity\n",
        "### Datasets used:\n",
        "1. Scraped Data from Politifact and FoxNews Website\n",
        "\n",
        "________\n",
        "## Team Musketeers\n",
        "### Raghava Devaraje Urs\n",
        "#### Microfactors\n",
        "**Political Affiliation**\n",
        "1. Sentiment analysis\n",
        "2. Party affiliations \n",
        "3. Click Bait\n",
        "\n",
        "### Kumuda Benakanahalli \n",
        "#### Microfactors\n",
        "**Spam**\n",
        "1. HAM WORD count\n",
        "2. SPAM WORD count \n",
        "3. Readability ease\n",
        "\n",
        "### Shiv Kumar Ganesh\n",
        "#### Microfactors\n",
        "**Writing Style**\n",
        "1. Vocab Analysis\n",
        "2. Lexical Analysis\n",
        "3. Readibility Analysis\n",
        "_____\n",
        "## Team ml-coders\n",
        "###  \n",
        "#### Microfactors\n",
        "1. Sentiment Intensity\n",
        "2. Political Bias\n",
        "3. Readability Score\n",
        "1. Clickbait\n",
        "2. Toxicity\n",
        "3. Subjectivity\n",
        "4. Sentiment Polarity\n",
        "1. Sensatonalism\n",
        "2. Linguistic Bias\n",
        "3. Vagnuess\n",
        "1. Sentiment Analysis\n",
        "2. Readibility Analysis\n",
        "\n",
        "\n",
        "_____\n",
        "## Team BTS\n",
        "### Tamanna\n",
        "### Factor- Intent\n",
        "#### Microfactors\n",
        "1. Credibility Score \n",
        "2. Sentiment Score \n",
        "3. Topic \n",
        "###Datasets used:\n",
        "1. Liar liar Plus dataset\n",
        "2. Scraped Data from Politifact and NYTimes\n",
        "\n",
        "### Bharath\n",
        "### Factor- Toxicity\n",
        "#### Microfactors\n",
        "\n",
        "1. Sentiment\n",
        "2. Emotions\n",
        "3. Bad Words\n",
        "\n",
        "### Datasets used:\n",
        " 1. Toxicity dataset\n",
        " 2. Scraped Data from Politifact and NYTimes\n",
        "\n",
        "### Stuti Agarwal\n",
        "\n",
        "### Factor-Clickbait\n",
        "\n",
        "####Micro Factors-\n",
        "\n",
        "1. Sensationalism,\n",
        "2. Length of headline phrases,\n",
        "3. word embeddings\n",
        "4. Word Contractions\n",
        "\n",
        "###Datasets used:\n",
        "1.Kaggle Clickbait dataset\n",
        "\n",
        "2.Scraped Data from Politifact and NYTimes\n",
        "_____\n",
        "\n",
        "\n",
        "\n",
        "## Team Ninjas\n",
        "### Rishitha Bandi\n",
        "#### Microfactors\n",
        "*Content Veracity*\n",
        "1. Structural features  \n",
        "2. Content features \n",
        "3. Emotional Score\n",
        "### Datasets used:\n",
        "pheme dataset( labelled twitter dataset), scraped from Twitter using tweepy\n",
        "\n",
        "### Shreya Goyal \n",
        "#### Microfactors\n",
        "*Social Credibility*\n",
        "1. Sentiment Polarity\n",
        "2. Topic Credibility\n",
        "3. Social factors (Retweet)\n",
        "#### Datasets used:\n",
        "Kaggle fake news dataset, Liar Liar datset, pheme dataset\n",
        "\n",
        "### Tripura\n",
        "#### Microfactors\n",
        "*News Coverage*\n",
        "1. Geographic spread \n",
        "2. Event importance\n",
        "3. Sentiment score\n",
        "#### Datasets used:\n",
        "Liar dataset, scraped from politifact ,foxnews , apinews, Google news\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C4abpQqjAj5"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt6Ge7dNq29P"
      },
      "source": [
        "# Import standard libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import pickle\n",
        "import nltk\n",
        "from transformers import pipeline\n",
        "nltk.download('punkt')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import gensim\n",
        "from scipy import sparse\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "import re\n",
        "from scipy import sparse\n",
        "nlp = en_core_web_sm.load()\n",
        "stopwords_list = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6JXxaedfOj7"
      },
      "source": [
        "!pip install -U -q pyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JElDUMYeEGBM"
      },
      "source": [
        "!pip install NRCLex\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWqJQotGeV6b"
      },
      "source": [
        "# Import packages for google drive, auth\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "gdrive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvzcys0ueBkd"
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from textblob import TextBlob\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XehGrHOx8ZeN"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq8WOCWL-sIP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noSA0zB6nzwJ"
      },
      "source": [
        "# 1.0. Read in streaming news headlines\n",
        "\n",
        "a. Streaming news headlines are from https://newsapi.org/  \n",
        "c. Streaming news headlines are from CNN, Brietbart News and Fox News 2021/4/25 - 2021/4/26"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEdR7Valn_H2"
      },
      "source": [
        "r = requests.get('https://docs.google.com/spreadsheets/d/e/2PACX-1vQoXVHhfQlxAlQ8b3eHot7dDhXmCYM9iYC7i0mZMMpzwejhvCjMeEEHPTRhI7KCqOkRbmHBfsxKp0gw/pub?gid=1486725861&single=true&output=tsv')\n",
        "data = r.content\n",
        "df_test_headlines = pd.read_csv(BytesIO(data), sep='\\t')\n",
        "df_test_headlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I47CwebudtV2"
      },
      "source": [
        "# 2.0. Predict news headline is true/ false by ensembling factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "timAm5UXdmqP"
      },
      "source": [
        "## 2.a. Define a false-o-meter\n",
        "1. Associate weights with each micro-factor proportional to model accuracy\n",
        "2. Probablity news is false is obtained by ensembling micro-factors as follows\n",
        "Define: A [false-o-meter] as s polynomial function f(p) = p0w0 + p1w1 + p2*w2\n",
        "where\n",
        "i. p is predicited probability of a micro-factor\n",
        "ii. w is normalized weight of micro-factors, proportional to accuracy of its prediction\n",
        "\n",
        "3. Labels news as follows based on false-o-meter f(p) reading\n",
        "i. Pants on Fire - if false-o-meter > 0.9\n",
        "ii. Somewhat False - if 0.7 < false-o-meter < 0.9\n",
        "iii. Mostly False - if 0.5 < false-o-meter < 0.7\n",
        "iv. Half True - if 0.3 < false-o-meter < 0.5\n",
        "v. Mostly True - if 0.1 < false-o-meter < 0.3\n",
        "vi. True - if 0.1 < false-o-meter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Tj9YbF3u6u5"
      },
      "source": [
        "## 2.b. Define Stance Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKq470C2grOd"
      },
      "source": [
        "def apply_stance_detection_featurization(df_, sentiment_analyzer, headlineCol='Headline', bodyCol='ArticleBody'):\n",
        "  orig_cols = df_.copy().columns\n",
        "  df_['body_sentiment_score'] = df_[bodyCol].apply(lambda text: sentiment_analyzer.polarity_scores(text)['compound'])\n",
        "  df_['body_subjectivity_score'] = df_[bodyCol].apply(lambda text: TextBlob(text).sentiment[1])\n",
        "  df_['title_sentiment_score'] = df_[headlineCol].apply(lambda text: sentiment_analyzer.polarity_scores(text)['compound'])\n",
        "  df_['title_subjectivity_score'] = df_[headlineCol].apply(lambda text: TextBlob(text).sentiment[1])\n",
        "  df_ = df_.reset_index()\n",
        "\n",
        "  feature_names = ['body_sentiment_score', 'body_subjectivity_score', 'title_sentiment_score', 'title_subjectivity_score']\n",
        "  poly = PolynomialFeatures(interaction_only=True)\n",
        "  interaction_features = pd.DataFrame(poly.fit_transform(df_[feature_names].to_numpy()))\n",
        "  interaction_feature_names = poly.get_feature_names(input_features=feature_names)\n",
        "  interaction_features.columns = interaction_feature_names\n",
        "  interaction_features = interaction_features.drop(['1'], axis=1)\n",
        "  interaction_feature_names.remove('1')\n",
        "\n",
        "  headline_sentence_embeddings = pd.DataFrame(np.stack(df_[headlineCol].apply(transformer_model.encode).to_numpy()), columns=['heademb_{}'.format(i) for i in range(768)])\n",
        "  article_sentence_embeddings = pd.DataFrame(np.stack(df_[bodyCol].apply(transformer_model.encode).to_numpy()), columns=['artemb_{}'.format(i) for i in range(768)])\n",
        "\n",
        "  features = pd.concat([df_[orig_cols], interaction_features, headline_sentence_embeddings, article_sentence_embeddings], axis=1)\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQuyQVdbmd0F"
      },
      "source": [
        "file_id = '1DV5hmLvLJWYBviF6ps0nXV5FCu1URXyL'\n",
        "model_filename = 'stance_detection.pkl'\n",
        "downloaded = gdrive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile(model_filename)\n",
        "pickle_filepath = '/content/{}'.format(model_filename)\n",
        "stance_detection_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "transformer_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "def getStancePrediction(X_headline, X_body):\n",
        "  prob = [0, 0, 0]\n",
        "  if X_headline.size == 1:\n",
        "    # prepare data for stance prediction\n",
        "    df = pd.DataFrame([(X_headline.iloc[0], X_body.iloc[0])], columns=['Headline', 'ArticleBody']) \n",
        "    stance_detection_features = apply_stance_detection_featurization(df, sentiment_analyzer, headlineCol='Headline', bodyCol='ArticleBody')\n",
        "    stance_detection_X = stance_detection_features.drop(['Headline', 'ArticleBody'], axis=1).to_numpy()\n",
        "    stance_detection_prediction_score = stance_detection_model.predict_proba(stance_detection_X)[0]\n",
        "    # make a prediction\n",
        "    prob = stance_detection_prediction_score\n",
        "\n",
        "  return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoz_GwWEuyOK"
      },
      "source": [
        "## 2.c. Define Sentiment Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4qVLO1EdqH2"
      },
      "source": [
        "def getSentimentPrediction(X_news):\n",
        "  prob = 0\n",
        "  if X_news.size == 1:\n",
        "    file_id = '1eZ0TycVjHAyaFh8eKDmyLiQ_DN8rOcbI'\n",
        "    model_filename = 'Best_Sentiment_Analysis_Model_Misleading_Intentions.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best_sentiment_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    prob = best_sentiment_model.predict_proba(X_news)[:,1]\n",
        "  return float(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x8PlEguu1s4"
      },
      "source": [
        "## 2.d. Define Sensationalism Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_MZQd_90cvu"
      },
      "source": [
        "def getSensationalismPrediction(X_news):\n",
        "  prob = 0\n",
        "  if X_news.size == 1:\n",
        "    file_id = '1XEYOqUEkI52tW7ZWtIGRq0Qe5dOd2I_S'\n",
        "    model_filename = 'Best_Sensationalism_Analysis_Model_Misleading_Intentions.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best_sensationalism_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    prob = best_sensationalism_model.predict_proba(X_news)[:,1]\n",
        "  return float(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoC7DQHSvDG6"
      },
      "source": [
        "## 2.e. Define ClickBait Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEEQtnWNd3Ic"
      },
      "source": [
        "def getDistilledClickBaitPrediction(X_news):\n",
        "  prob = 0\n",
        "  if X_news.size == 1:\n",
        "    file_id = '1pgSrMJD0m_7Cd1fg1xoZEN2P_CjnpUkb'\n",
        "    model_filename = 'Best_Clickbait_Analysis_Model_Misleading_Intentions.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best_distilled_clickbait_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    prob = best_distilled_clickbait_model.predict_proba(X_news)[:,1]\n",
        "  return float(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7YJG4qERpqA"
      },
      "source": [
        "## 2.f. Define Political Bias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ_fD6l_R2n9"
      },
      "source": [
        "def get_BSF(df): # Balance Sentiment Factor\n",
        "  if len(df) == 1:\n",
        "    df['BSF'] = df['Positive']/df['Negative']\n",
        "  else:\n",
        "    pos_mean = df['Positive'].mean()\n",
        "    neg_mean = df['Negative'].mean()\n",
        "    balance_sentiment = (abs(pos_mean - df['Positive'])+abs(neg_mean - df['Negative']))/2\n",
        "    df['BSF'] = balance_sentiment \n",
        "  return df\n",
        "\n",
        "# Microfactor 2\n",
        "def get_SPR(df): # Standardized Party Ratio\n",
        "  # Create a ratio to measure if text has a leniency towards a particular party\n",
        "  party_ratio = df['Democrat']/df['Republican']\n",
        "  # Standardized the ratio to make use of the overall mean and stand deviation\n",
        "  if len(df) == 1:\n",
        "    df['SPR'] = party_ratio\n",
        "  else:\n",
        "    df['SPR'] = abs(party_ratio - np.mean(party_ratio))/np.std(party_ratio)\n",
        "  return df\n",
        "\n",
        "# Microfactor 3\n",
        "def get_selection_bias(df, text_col=str): \n",
        "  clean_col_name = 'Cleaned_'+text_col\n",
        "  clean_text_token = df[text_col].apply(nltk.word_tokenize)\n",
        "\n",
        "  def count_bias_vocab(target, bias_list):\n",
        "    count = 0 \n",
        "    for vocab in bias_list:\n",
        "      if vocab in target:\n",
        "        count += 1\n",
        "    return count/len(bias_list)\n",
        "  \n",
        "  file_id = '15DBBkgI0TVfciwwhDptWoblvhIHGKmq9'\n",
        "  model_filename = 'vocab_selection.pkl'\n",
        "  downloaded = gdrive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(model_filename)\n",
        "  pickle_filepath = '/content/{}'.format(model_filename)\n",
        "  vocab_selection = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "  lib_vocab_rate = clean_text_token.apply(count_bias_vocab,bias_list=vocab_selection['liberal'])\n",
        "  con_vocab_rate = clean_text_token.apply(count_bias_vocab,bias_list=vocab_selection['conservative'])\n",
        "  dem_vocab_rate = clean_text_token.apply(count_bias_vocab,bias_list=vocab_selection['democrat'])\n",
        "  rep_vocab_rate = clean_text_token.apply(count_bias_vocab,bias_list=vocab_selection['republican'])\n",
        "\n",
        "  # Create two new feature\n",
        "  # Weight more on liberal and conservative vocabs\n",
        "  df['Dem_Vocab_Freq'] = 0.6*lib_vocab_rate+0.4*dem_vocab_rate\n",
        "  df['Rep_Vocab_Freq'] = 0.6*con_vocab_rate+0.4*rep_vocab_rate\n",
        "  # Create a selection bias feature\n",
        "  df['Selection_Bias'] = df[[\"Dem_Vocab_Freq\", \"Rep_Vocab_Freq\"]].max(axis=1)\n",
        "  return df\n",
        "\n",
        "# Combine all microfactors\n",
        "def get_political_bias(df):\n",
        "  # Microfactor final calculation\n",
        "  if len(df) == 1:\n",
        "    BSF_diff = df['BSF']\n",
        "    SPR_diff = df['SPR']\n",
        "  else:\n",
        "    BSF_diff = abs(df['BSF'].mean() - df['BSF'])\n",
        "    BSF_diff = (BSF_diff-min(BSF_diff))/(max(BSF_diff)-min(BSF_diff))\n",
        "    SPR_diff = abs(df['SPR'].mean() - df['SPR'])\n",
        "    SPR_diff = (SPR_diff-min(SPR_diff))/(max(SPR_diff)-min(SPR_diff))\n",
        "  # Combine all the microfactors together\n",
        "  political_bias = (0.2*BSF_diff+0.2*SPR_diff+0.2*(1-df['Neutral'])+0.4*df['Selection_Bias'])\n",
        "  df['Political_Bias'] = political_bias\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h046_6ZUkrtn"
      },
      "source": [
        "To save time on loading zero shot model and create microfactors based on overall dataframe statistics, feature generation process (zero_shot_microfactors) is added in data prep notebook "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWHV-rlDSBij"
      },
      "source": [
        "def polit_bias_pipeline(df, clean_text_col=str):\n",
        "  #df = zero_shot_microfactor(df, text_col) \n",
        "  df = get_BSF(df)\n",
        "  df = get_SPR(df)\n",
        "  df = get_selection_bias(df, clean_text_col)\n",
        "  df = get_political_bias(df)\n",
        "  return df, df['Political_Bias']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLx1PK9oogYU"
      },
      "source": [
        "## 2.g. Title-Body Similarity Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z5hxsa5oyNj"
      },
      "source": [
        "!pip install nltk==3.4 --quiet\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from scipy.sparse import vstack\n",
        "from scipy.spatial.distance import cosine\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def cnt_sentences(df):\n",
        "  df['cnt_title_sentences'] = df['clean_title'].apply(lambda x: len(sent_tokenize(x)))\n",
        "  df['cnt_text_sentences'] = df['clean_body'].apply(lambda x: len(sent_tokenize(x)))\n",
        "\n",
        "def ngram(text, n):\n",
        "    n_grams = ngrams(word_tokenize(text), n)\n",
        "    return [ '_'.join(grams) for grams in n_grams]\n",
        "\n",
        "# Uni, Bi, Tri grams to get common word count features\n",
        "def generate_ngrams(df):\n",
        "  df[\"title_uni\"] = df[\"clean_title\"].map(lambda x: ngram(x, 1))\n",
        "  df[\"body_uni\"] = df[\"clean_body\"].map(lambda x: ngram(x, 1))\n",
        "  df[\"cnt_title_uni\"] = list(df.apply(lambda x: len(x['title_uni']), axis=1))\n",
        "  df[\"cnt_body_uni\"] = list(df.apply(lambda x: len(x['body_uni']), axis=1))\n",
        "  df[\"unq_cnt_title_uni\"] = list(df.apply(lambda x: len(set(x['title_uni'])), axis=1))\n",
        "  df[\"unq_cnt_body_uni\"] = list(df.apply(lambda x: len(set(x['body_uni'])), axis=1))\n",
        "\n",
        "  df[\"title_bi\"] = df[\"clean_title\"].map(lambda x: ngram(x, 2))\n",
        "  df[\"body_bi\"] = df[\"clean_body\"].map(lambda x: ngram(x, 2))\n",
        "  df[\"cnt_title_bi\"] = list(df.apply(lambda x: len(x['title_bi']), axis=1))\n",
        "  df[\"cnt_body_bi\"] = list(df.apply(lambda x: len(x['body_bi']), axis=1))\n",
        "  df[\"unq_cnt_title_bi\"] = list(df.apply(lambda x: len(set(x['title_bi'])), axis=1))\n",
        "  df[\"unq_cnt_body_bi\"] = list(df.apply(lambda x: len(set(x['body_bi'])), axis=1))\n",
        "\n",
        "  df[\"title_tri\"] = df[\"clean_title\"].map(lambda x: ngram(x, 3))\n",
        "  df[\"body_tri\"] = df[\"clean_body\"].map(lambda x: ngram(x, 3))\n",
        "  df[\"cnt_title_tri\"] = list(df.apply(lambda x: len(x['title_tri']), axis=1))\n",
        "  df[\"cnt_body_tri\"] = list(df.apply(lambda x: len(x['body_tri']), axis=1))\n",
        "  df[\"unq_cnt_title_tri\"] = list(df.apply(lambda x: len(set(x['title_tri'])), axis=1))\n",
        "  df[\"unq_cnt_body_tri\"] = list(df.apply(lambda x: len(set(x['body_tri'])), axis=1))\n",
        "\n",
        "def common_ngrams_in_body(df):\n",
        "  df[\"cnt_title_unis_in_body\"] =  list(df.apply(lambda x: sum([1. for w in x['title_uni'] if w in set(x['body_uni'])]), axis=1))\n",
        "  df[\"cnt_title_bis_in_body\"] =  list(df.apply(lambda x: sum([1. for w in x['title_bi'] if w in set(x['body_bi'])]), axis=1))\n",
        "  df[\"cnt_title_tris_in_body\"] =  list(df.apply(lambda x: sum([1. for w in x['title_tri'] if w in set(x['body_tri'])]), axis=1))\n",
        "\n",
        "def concat_title_body(df):\n",
        "  df['clean_title_body'] = df['clean_title'] + ' ' + df['clean_body']\n",
        "\n",
        "def tf_idf(df):\n",
        "  concat_title_body(df)\n",
        "  combined_vectors = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=1, use_idf=True, smooth_idf=True)\n",
        "  combined_vectors.fit(df[\"clean_title_body\"])\n",
        "  combined_vectors_dictionary = combined_vectors.vocabulary_\n",
        "  title_vectors = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=1, use_idf=True, smooth_idf=True, vocabulary=combined_vectors_dictionary)\n",
        "  title_tfidf_vectors = title_vectors.fit_transform(df['clean_title'])\n",
        "  text_vectors = TfidfVectorizer(ngram_range=(1, 2), min_df=1, max_df=1, use_idf=True, smooth_idf=True, vocabulary=combined_vectors_dictionary)\n",
        "  text_tfidf_vectors = text_vectors.fit_transform(df['clean_body'])\n",
        "  return title_tfidf_vectors, text_tfidf_vectors\n",
        "\n",
        "def similarity_score(df, title_vectors, text_vectors):\n",
        "  similarity_score = []\n",
        "  for i in range(len(df)):\n",
        "      similarity_score.append(1 - cosine(title_vectors[i], text_vectors[i]))\n",
        "  return similarity_score\n",
        "\n",
        "def tf_idf_similarity(df):\n",
        "  title_tfidf_vectors, text_tfidf_vectors = tf_idf(df)\n",
        "  df['similarity_title_body'] = similarity_score(df, title_tfidf_vectors.toarray(), text_tfidf_vectors.toarray())\n",
        "  return title_tfidf_vectors, text_tfidf_vectors\n",
        "\n",
        "def svd(data, title_tfidf_vectors, text_tfidf_vectors):\n",
        "  truncated_svd = TruncatedSVD(n_components=2, n_iter=10)\n",
        "  combined_vectors = vstack([title_tfidf_vectors, text_tfidf_vectors])\n",
        "  truncated_svd.fit(combined_vectors)\n",
        "  title_svd = truncated_svd.transform(title_tfidf_vectors)\n",
        "  text_svd = truncated_svd.transform(text_tfidf_vectors)\n",
        "  return title_svd, text_svd\n",
        "\n",
        "def topic_similarity(data, title_tfidf_vectors, text_tfidf_vectors):\n",
        "  title_svd_vectors, text_svd_vectors = svd(data, title_tfidf_vectors, text_tfidf_vectors)\n",
        "  data['topics_similarity_title_body'] = similarity_score(data, title_svd_vectors, text_svd_vectors)\n",
        "\n",
        "def get_distilled_dataset(title, text):\n",
        "  data = {'clean_title': [title.iloc[0]], 'clean_body': [text.iloc[0]]}\n",
        "  df_test = pd.DataFrame(data)\n",
        "  cnt_sentences(df_test)\n",
        "  generate_ngrams(df_test)\n",
        "  common_ngrams_in_body(df_test)\n",
        "  title_tfidf_vectors, text_tfidf_vectors = tf_idf_similarity(df_test)\n",
        "  topic_similarity(df_test, title_tfidf_vectors, text_tfidf_vectors)\n",
        "  X_cols = [x for i,x in enumerate(features) if x!='label']\n",
        "  return df_test[X_cols]\n",
        "\n",
        "features =     ['label',  'cnt_title_uni', 'cnt_body_uni',\n",
        "                'unq_cnt_title_uni', 'unq_cnt_body_uni', 'cnt_title_bi', 'cnt_body_bi',\n",
        "                'unq_cnt_title_bi', 'unq_cnt_body_bi', 'cnt_title_tri', 'cnt_body_tri',\n",
        "                'unq_cnt_title_tri', 'unq_cnt_body_tri', 'cnt_title_unis_in_body', \n",
        "                'cnt_title_bis_in_body', 'cnt_title_tris_in_body', 'similarity_title_body',\n",
        "                'topics_similarity_title_body',\n",
        "                ]\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(['agree', 'disagree', 'discuss', 'unrelated'])\n",
        "dict(zip(le.classes_, le.transform(le.classes_)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXwY8Kgm1V0Q"
      },
      "source": [
        "def getTitleVsBodyPrediction(title, body):\n",
        "  file_id = '1bwvFThCwg6pgM99R6p7Ly7K5vXPwRj6q'\n",
        "  model_filename = 'title_body_similarity_model.pkl'\n",
        "  downloaded = gdrive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(model_filename)\n",
        "  pickle_filepath = '/content/{}'.format(model_filename)\n",
        "  title_body_similarity_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "  df_test = get_distilled_dataset(title, body)\n",
        "  return title_body_similarity_model.predict(df_test), title_body_similarity_model.predict_proba(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkCoAfpDHtWX"
      },
      "source": [
        "#Define Spam Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7TeYFFKCHl"
      },
      "source": [
        "pip install textstat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmydG_ZmHpYe"
      },
      "source": [
        "from joblib import dump,load\n",
        "def load_ham_model():\n",
        "  !cp '/content/drive/MyDrive/MLSpring-2021/TeamIntegration_MLSpring2021/models/Kumuda _SpamFactor/ham_vectorizer' -d /content/ham_vectorizer\n",
        "  ! cp  '/content/drive/MyDrive/MLSpring-2021/TeamIntegration_MLSpring2021/models/Kumuda _SpamFactor/ham_classifier.model' -d /content/ham_classifier.model\n",
        "  ham_classifier=load('/content/ham_classifier.model')\n",
        "  ham_vectorizer=load('/content/ham_vectorizer')\n",
        "  return ham_vectorizer,ham_classifier\n",
        "\n",
        "def load_spam_model():\n",
        "  !cp '/content/drive/MyDrive/MLSpring-2021/TeamIntegration_MLSpring2021/models/Kumuda _SpamFactor/spam_vectorizer' -d /content/spam_vectorizer\n",
        "  ! cp  '/content/drive/MyDrive/MLSpring-2021/TeamIntegration_MLSpring2021/models/Kumuda _SpamFactor/spam_classifier.model' -d /content/spam_classifier.model\n",
        "  spam_classifier=load('/content/spam_classifier.model')\n",
        "  spam_vectorizer=load('/content/spam_vectorizer')\n",
        "  return spam_vectorizer,spam_classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u27PWskSH36E"
      },
      "source": [
        "ham_vectorizer,ham_classifier=load_ham_model()  \n",
        "spam_vectorizer,spam_classifier=load_spam_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIVVk-9H8zF"
      },
      "source": [
        "def calculate_ham_score(X_news):\n",
        "  X_train=ham_vectorizer.transform(X_news)\n",
        "  ham_score=ham_classifier.predict_proba(X_train)[:,1]\n",
        "  return float(ham_score[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A21RlQwhIEbv"
      },
      "source": [
        "def calculate_spam_score(X_news):\n",
        "  X_train=spam_vectorizer.transform(X_news)\n",
        "  spam_score=spam_classifier.predict_proba(X_train)[:,1]\n",
        "  return float(spam_score[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZQ_i9ZbIB6T"
      },
      "source": [
        "def get_reading_ease(X_news):\n",
        "  reading_ease=0.0\n",
        "  reading_ease=X_news.apply(textstat.flesch_reading_ease)\n",
        "  return float(reading_ease)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U28pLtzPIHJU"
      },
      "source": [
        "import textstat\n",
        "def generateSpamScore(X_news):\n",
        "  accuracy = [0.4,0.4, 0.001]\n",
        "  w = [float(i)/sum(accuracy) for i in accuracy]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  ham_score=calculate_ham_score(X_news)\n",
        "  prob.append(w[0] *(1- ham_score))#Ham Score\n",
        "  sumW =sumW + w[0]\n",
        "\n",
        "  spam_score=calculate_spam_score(X_news)\n",
        "  prob.append(w[1] * spam_score)#Spam Score\n",
        "  sumW += w[1]\n",
        "  prob.append(w[2] * get_reading_ease(X_news)) #Reading Ease\n",
        "  sumW += w[2]\n",
        "   \n",
        "  probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "  return probTotal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpAG3S6dvHc5"
      },
      "source": [
        "## 2.h. Define a false-o-meter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ufpr5uDd5sE"
      },
      "source": [
        "# get false-o-meter reading of news item\n",
        "def get_false_o_meter_reading(df, index_num, x_news=str, x_body=str):\n",
        "    X_news = df[x_news]\n",
        "    X_body = df[x_body]\n",
        "    model_accuracy = [0.85, 0.73, 0.89, 0.92, 0.86, 0.97, 0.6, 0.7,0.83,0.69,0.30, 0.60] \n",
        "    model_weight = [acc/sum(model_accuracy) for acc in model_accuracy]\n",
        "    probablity_false_news = []\n",
        "\n",
        "    # get sentiment reading\n",
        "    sentiment_prob = getSentimentPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[0] * sentiment_prob)\n",
        "    print('Sentiment [false-o-meter] reading is %f ' %(sentiment_prob))\n",
        "\n",
        "    # get sensationalism reading\n",
        "    sensationalism_prob = getSensationalismPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[1] * sensationalism_prob)\n",
        "    print('Sensationalism [false-o-meter] reading is %f ' %(sensationalism_prob))\n",
        "\n",
        "    # get distilled clickbait reading\n",
        "    clickbait_prob = getDistilledClickBaitPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[2] * clickbait_prob)\n",
        "    print('Distilled Clickbait [false-o-meter] reading is %f ' %(clickbait_prob))\n",
        "\n",
        "    # get stance reading\n",
        "    (agree_stance_prob, disagree_stance_prob, neutral_stance_prob) = getStancePrediction(X_news, X_body)\n",
        "    probablity_false_news.append(model_weight[3] * agree_stance_prob) # agree stance\n",
        "    probablity_false_news.append(model_weight[4] * disagree_stance_prob) # disagree stance\n",
        "    probablity_false_news.append(model_weight[5] * neutral_stance_prob) # neutral stance\n",
        "    print('Stance [false-o-meter] reading is (agree: %f, disagree: %f, neutral: %f)' % (agree_stance_prob, disagree_stance_prob, neutral_stance_prob))\n",
        "\n",
        "    # get political bias\n",
        "    r = requests.get('https://docs.google.com/spreadsheets/d/e/2PACX-1vQd6WhaekUPRDxUIYXgx_zI_zAHodXl3__bfAnEa_GWT_eR9dVO55HALi_3jjnZmEwbZ_4YvUkG7Qtx/pub?gid=896471122&single=true&output=tsv')\n",
        "    data = r.content\n",
        "    zero_shot_microfactors = pd.read_csv(BytesIO(data), sep='\\t')\n",
        "    pb_df, political_bias_prob = polit_bias_pipeline(zero_shot_microfactors, x_news)\n",
        "    probablity_false_news.append(model_weight[6] * political_bias_prob.loc[index_num])\n",
        "    print('Political Bias [false-o-meter] reading is %f ' %(political_bias_prob.loc[index_num]))\n",
        "\n",
        "    # get title body similarity\n",
        "    title_body_pred, title_body_pred_prob = getTitleVsBodyPrediction(X_news, X_body)\n",
        "    t_b_fake_score = (title_body_pred_prob[0][1] * 0.6 + title_body_pred_prob[0][3] + 0.4)\n",
        "    probablity_false_news.append(model_weight[6] * t_b_fake_score)\n",
        "    print('Title-Body incongruence [false-o-meter] reading is %f ' %(t_b_fake_score))\n",
        "\n",
        "    # get distilled psychology utility reading\n",
        "    psychology_prob = getPsychologyUtilitiesPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[2] * psychology_prob)\n",
        "    print('Distilled Psychology Utility [false-o-meter] reading is %f ' %(psychology_prob))\n",
        "\n",
        "    # get distilled intent reading\n",
        "    intent_prob = getIntentPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[0] * intent_prob)\n",
        "    print('Distilled Intent [false-o-meter] reading is %f ' %(intent_prob))\n",
        "\n",
        "    # get spam score\n",
        "    spam_score= generateSpamScore(X_news)\n",
        "    probablity_false_news.append(model_weight[8] * spam_score)\n",
        "    print('Spam score is %f' %(spam_score))\n",
        "\n",
        "    #get bts clickbait reading\n",
        "    bts_clickbait_prob = getBTSclickbaitPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[5] * bts_clickbait_prob)\n",
        "    print('BTS Clickbait [false-o-meter] reading is %f ' %(bts_clickbait_prob))\n",
        "    \n",
        "    # get distilled intent factor reading\n",
        "    intent_prob_bts = getDistilledIntentPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[9] * intent_prob_bts)\n",
        "    print('Distilled Intent_bts_ [false-o-meter] reading is %f ' %(intent_prob_bts))\n",
        "  \n",
        "    # get bts toxicity reading\n",
        "    bts_toxicity_prob = getBTSToxicityPrediction(X_news)\n",
        "    probablity_false_news.append(model_weight[10] * bts_toxicity_prob)\n",
        "    print('BTS toxicity [false-o-meter] reading is %f ' %(bts_toxicity_prob))\n",
        "\n",
        "    # # to get Content veracity, social credibility and news coverage scores\n",
        "    # ninjas = Testing(df,x_news)\n",
        "    # probablity_false_news.append(model_weight[0] * ninjas)\n",
        "    # print('Ninjas [false-o-meter] reading is %f ' %(ninjas))\n",
        "\n",
        "    # get writing style score\n",
        "    writing_score= writing_style_score(X_news.tolist()[0])\n",
        "    probablity_false_news.append(model_weight[9] * writing_score)\n",
        "    print('Writing Style score is %f' %(writing_score))\n",
        "\n",
        "    # get Party Affiliation score\n",
        "    political_affiliation_score = generatePoliticalAffiliationScore(x_news , X_news)\n",
        "    probablity_false_news.append(model_weight[11] * political_affiliation_score)\n",
        "    print('Political Affiliation score is %f' %(political_affiliation_score))\n",
        "\n",
        "     # get style factor\n",
        "    style_proba = getStyleFactor(X_body)\n",
        "    print('Distilled Style [false-o-meter] reading is %f ' %(style_proba))\n",
        "    probablity_false_news.append(model_weight[11] * style_proba)\n",
        "\n",
        "    cummalative_probablity_false_news = sum(probablity_false_news)\n",
        "    print('Ensembled [false-o-meter] reading is %f ' %(cummalative_probablity_false_news))\n",
        "\n",
        "    return cummalative_probablity_false_news"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbMdc5tLoT7p"
      },
      "source": [
        "##2.i. Define Psychology Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A4Zueetoc7p"
      },
      "source": [
        "import string\n",
        "import joblib\n",
        "\n",
        "def get_text_processing(text):\n",
        "  stop_words = stopwords.words('english')\n",
        "  stop_words.append(['breaking', 'BREAKING'])\n",
        "  no_punctuation = [char for char in text if char not in string.punctuation]\n",
        "  no_punctuation = ''.join(no_punctuation)\n",
        "  return ' '.join([word for word in no_punctuation.split() if word.lower() not in stop_words])\n",
        "\n",
        "def getPsychologyUtilitiesPrediction(X_news):\n",
        "  prob = 0\n",
        "  X_news = X_news.apply(get_text_processing)\n",
        "  if X_news.size == 1:\n",
        "    file_id = '16egOQ8zTftur5jPFxfWTwYDTOOjdhQ6e'\n",
        "    model_filename = 'PsychologyUtilites_pipeline.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best_distilled_psychology_model = joblib.load(open(pickle_filepath, 'rb'))\n",
        "    prob = best_distilled_psychology_model.predict(X_news)\n",
        "  return 1 if prob == 'Positive' else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUfbWvK2OBZ5"
      },
      "source": [
        "##2.j. Define Intent Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Iwlj7wOBZ7"
      },
      "source": [
        "def getIntentPrediction(X_news):\n",
        "  prob = 0\n",
        "  X_news = X_news.apply(get_text_processing)\n",
        "  if X_news.size == 1:\n",
        "    file_id = '1BFXgdw2MvJZl39CUx0jvfms1nzQI810j'\n",
        "    model_filename = 'Intent_pipeline.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best_distilled_intent_model = joblib.load(open(pickle_filepath, 'rb'))\n",
        "    prob = best_distilled_intent_model.predict(X_news)\n",
        "  return 1 if prob == 'Positive' else 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOqDc2R51490"
      },
      "source": [
        "##Team BTS Factors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHRgmRNuncmR"
      },
      "source": [
        "##2.k. Define BTS Clickbait Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sj5I-hxqiV2"
      },
      "source": [
        "def clean_text(text):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  text = re.sub(cleanr, ' ', str(text))\n",
        "  tokens = word_tokenize(text)\n",
        "  # convert to lower case\n",
        "  tokens = [w.lower() for w in tokens]\n",
        "  # remove punctuation from each word\n",
        "  import string\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  words = [word for word in stripped if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  from nltk.corpus import stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  return \" \".join(words)\n",
        "#helper utility to find if string contains a question and if so, update new feature with a 1 for yes or 0 for no\n",
        "question_words = ['who', 'whos', 'whose', 'what', 'whats', 'whatre', 'when', 'whenre', 'whens', 'couldnt',\n",
        "                  'where', 'wheres', 'whered', 'why', 'whys', 'can', 'cant', 'could', 'will', 'would', 'is',\n",
        "                 'isnt', 'should', 'shouldnt', 'you', 'your', 'youre', 'youll', 'youd', 'here', 'heres',\n",
        "                  'how', 'hows', 'howd', 'this', 'are', 'arent', 'which', 'does', 'doesnt']\n",
        "\n",
        "def contains_question(headline):\n",
        "    if \"?\" in headline or headline.startswith(('who','what','where','why','when','whose','whom','would','will','how','which','should','could','did','do')):\n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "#helper utility to find if headline contains '!' and create new feature with 1 for yes and 0 for no\n",
        "def contains_exclamation(headline):\n",
        "    if \"!\" in headline: \n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "\n",
        "#helper utility to find if headline starts with a digit and create new feature with 1 for yes and 0 for no\n",
        "def starts_with_num(headline):\n",
        "    if headline.startswith(('1','2','3','4','5','6','7','8','9')): \n",
        "        return 1\n",
        "    else: \n",
        "        return 0\n",
        "contractions = ['tis', 'aint', 'amnt', 'arent', 'cant', 'couldve', 'couldnt', 'couldntve',\n",
        "                'didnt', 'doesnt', 'dont', 'gonna', 'gotta', 'hadnt', 'hadntve', 'hasnt',\n",
        "                'havent', 'hed', 'hednt', 'hedve', 'hell', 'hes', 'hesnt', 'howd', 'howll',\n",
        "                'hows', 'id', 'idnt', 'idntve', 'idve', 'ill', 'im', 'ive', 'ivent', 'isnt',\n",
        "                'itd', 'itdnt', 'itdntve', 'itdve', 'itll', 'its', 'itsnt', 'mightnt',\n",
        "                'mightve', 'mustnt', 'mustntve', 'mustve', 'neednt', 'oclock', 'ol', 'oughtnt',\n",
        "                'shant', 'shed', 'shednt', 'shedntve', 'shedve', 'shell', 'shes', 'shouldve',\n",
        "                'shouldnt', 'shouldntve', 'somebodydve', 'somebodydntve', 'somebodys',\n",
        "                'someoned', 'someonednt', 'someonedntve', 'someonedve', 'someonell', 'someones',\n",
        "                'somethingd', 'somethingdnt', 'somethingdntve', 'somethingdve', 'somethingll',\n",
        "                'somethings', 'thatll', 'thats', 'thatd', 'thered', 'therednt', 'theredntve',\n",
        "                'theredve', 'therere', 'theres', 'theyd', 'theydnt', 'theydntve', 'theydve',\n",
        "                'theydvent', 'theyll', 'theyontve', 'theyre', 'theyve', 'theyvent', 'wasnt',\n",
        "                'wed', 'wedve', 'wednt', 'wedntve', 'well', 'wontve', 'were', 'weve', 'werent',\n",
        "                'whatd', 'whatll', 'whatre', 'whats', 'whatve', 'whens', 'whered', 'wheres',\n",
        "                'whereve', 'whod', 'whodve', 'wholl', 'whore', 'whos', 'whove', 'whyd', 'whyre',\n",
        "                'whys', 'wont', 'wontve', 'wouldve', 'wouldnt', 'wouldntve', 'yall', 'yalldve',\n",
        "                'yalldntve', 'yallll', 'yallont', 'yallllve', 'yallre', 'yallllvent', 'yaint',\n",
        "                'youd', 'youdve', 'youll', 'youre', 'yourent', 'youve', 'youvent']\n",
        "\n",
        "def num_contract(text):\n",
        "    s = text.split()\n",
        "    num = len([word for word in s if word in contractions])\n",
        "    return num\n",
        "\n",
        "def name_entity_recognition(corpus):\n",
        "  ner_count = []\n",
        "  for index, row in corpus.iterrows():\n",
        "      valid_labels = ['PERSON','NORP','ORG','GPE','LANGUAGE','EVENT','LAW']\n",
        "      if index % 10 == 0:\n",
        "          print(index,\"/\",corpus.shape[0])\n",
        "      doc = nlp(row['headline'])\n",
        "      labels = [(X.label_) for X in doc.ents]\n",
        "      if len(set(labels).intersection(set(valid_labels))) > 0:\n",
        "        ner_count.append(1)\n",
        "      else:\n",
        "        ner_count.append(0)\n",
        "  return ner_count\n",
        "\n",
        "def vectorizing(dataset):\n",
        "  dataset = dataset.dropna()\n",
        "  sentences_train = dataset['headline'].apply(lambda x: x.split())\n",
        "  vectorizer =  TfidfVectorizer(stop_words = stopwords_list,ngram_range = (1,2),max_features=2)\n",
        "  tfidf_train = vectorizer.fit_transform(dataset['headline'].apply(lambda x: np.str_(x)))\n",
        "  X_train_ef = dataset.drop(columns='headline')\n",
        "  X_train = sparse.hstack([X_train_ef, tfidf_train]).tocsr()\n",
        "  return X_train\n",
        "\n",
        "def vectorizing_toxicity(dataset):\n",
        "  dataset = dataset.dropna()\n",
        "  sentences_train = dataset['cleaned_text'].apply(lambda x: x.split())\n",
        "  vectorizer =  TfidfVectorizer(max_features=1)\n",
        "  tfidf_train = vectorizer.fit_transform(dataset['cleaned_text'].apply(lambda x: np.str_(x)))\n",
        "  X_train_ef = dataset.drop(columns='cleaned_text')\n",
        "  X_train = sparse.hstack([X_train_ef, tfidf_train]).tocsr()\n",
        "  return tfidf_train\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twHIt0MzqDkW"
      },
      "source": [
        "def getClickbaitReady(dataset):\n",
        "  clickbait_data_df=pd.DataFrame({ 'headline': dataset })\n",
        "  clickbait_data_df['headline'] = dataset\n",
        "  clickbait_data_df.astype(str)\n",
        "  clickbait_data_df = clickbait_data_df.dropna()\n",
        "  #clickbait_data_df['headline'] = clickbait_data_df['headline'].apply(clean_text)\n",
        "  clickbait_data_df['headline']=clickbait_data_df['headline'].apply(lambda x: x.lower())\n",
        "  clickbait_data_df = clickbait_data_df.dropna()\n",
        "  clickbait_data_df['question']=clickbait_data_df['headline'].apply(contains_question)\n",
        "  clickbait_data_df['exclamation']=clickbait_data_df['headline'].apply(contains_exclamation)\n",
        "  clickbait_data_df['starts_with_num']=clickbait_data_df['headline'].apply(starts_with_num)\n",
        "  clickbait_data_df['headline_phrase_length'] = clickbait_data_df['headline'].apply(lambda x: len(x.split()))\n",
        "  clickbait_data_df = clickbait_data_df[clickbait_data_df['headline_phrase_length'] != 0]\n",
        "  clickbait_data_df['num_contract']=clickbait_data_df['headline'].apply(num_contract)\n",
        "  clickbait_data_df['NER_entity_count'] = name_entity_recognition(clickbait_data_df)\n",
        "  features = clickbait_data_df\n",
        "  X_train= vectorizing(clickbait_data_df)\n",
        "  return X_train\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o44kg1P_nhKi"
      },
      "source": [
        "\n",
        "def getBTSclickbaitPrediction(X_news):\n",
        "  prob = 0\n",
        "  if X_news.size == 1: \n",
        "    X_train = getClickbaitReady(X_news)\n",
        "    file_id = '1P30EODEhgm-cBtXrvbmtp9g0nnm8azQ6'\n",
        "    model_filename = 'clickbait model.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    clickbait_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    clickbait_result = clickbait_model.predict(X_train)\n",
        "    predict_proba_score  = clickbait_model.predict_proba(X_train)[:,1]\n",
        "    # make a prediction\n",
        "    prob = predict_proba_score\n",
        "  return float(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YPF5hNDtq7U"
      },
      "source": [
        "##2.l. Define BTS Toxicity Predictor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k8E8Oqv9DJb"
      },
      "source": [
        "!pip install afinn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhjUVjB0LHfp"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from afinn import Afinn\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "afn = Afinn()\n",
        "sid = SentimentIntensityAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLh74sFO7OJd"
      },
      "source": [
        "!ls /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0ELyKS6LSZe"
      },
      "source": [
        "def add_sentiment(row):\n",
        "  text = row['cleaned_text']\n",
        "  vader = sid.polarity_scores(text)\n",
        "  afn_score = afn.score(text)\n",
        "  row['vader'] = vader['compound']\n",
        "  row['afin'] = afn_score\n",
        "  return row\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqLhL_fkLUQS"
      },
      "source": [
        "def add_emotion(row):\n",
        "  text = row['cleaned_text']\n",
        "  emotion = NRCLex(text)\n",
        "  emotions = emotion.raw_emotion_scores\n",
        "  for key,value in emotions.items():\n",
        "    row[key] = value\n",
        "  return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA3lzTD1LXQj"
      },
      "source": [
        "def createWord2Vec(data):\n",
        "    # Set values for various parameters\n",
        "    num_features = 1    # Word vector dimensionality                      \n",
        "    min_word_count = 1   # Minimum word count                        \n",
        "    num_workers = 4       # Number of threads to run in parallel\n",
        "    context = 2          # Context window size                                                                                    \n",
        "    downsampling = 1e-3   # Downsample setting for frequent words\n",
        "    # Initialize and train the model (this will take some time)\n",
        "    model = word2vec.Word2Vec(data, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "    model.init_sims(replace=True) # marks the end of training to speed up the use of the model\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKi-8NjGLZ62"
      },
      "source": [
        "def makeFeatureVec(words, model, num_features):\n",
        "    # Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
        "    #\n",
        "    nwords = 0\n",
        "    # \n",
        "    # Index2word is a list that contains the names of the words in \n",
        "    # the model's vocabulary. Convert it to a set, for speed \n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    #\n",
        "    # Loop over each word in the review and, if it is in the model's\n",
        "    # vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set: \n",
        "            nwords = nwords + 1\n",
        "            featureVec = np.add(featureVec, model[word])\n",
        "    # Divide the result by the number of words to get the average\n",
        "    if nwords == 0:\n",
        "        nwords = 1\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec\n",
        "\n",
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "    # Given a set of reviews (each one a list of words), calculate \n",
        "    # the average feature vector for each one and return a 2D numpy array \n",
        "    # Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
        "    counter = 0\n",
        "    # Loop through the reviews\n",
        "    for review in reviews:\n",
        "        # Call the function (defined above) that makes average feature vectors\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "# f_matrix_train = getAvgFeatureVecs(sentences_train, model, num_features)\n",
        "# f_matrix_test = getAvgFeatureVecs(sentences_test, model, num_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEx0XPYsLcUV"
      },
      "source": [
        "def create_bad_word_corpus():\n",
        "    from io import BytesIO\n",
        "    r = requests.get('https://docs.google.com/spreadsheets/d/e/2PACX-1vSdJJyTEW0KcfI19Zd16ikRcdrpahYYZiEpLtFd7jriapnRuXBFa6gSYQxtXjprToAieODFR51BIwnB/pub?gid=694973728&single=true&output=csv')\n",
        "    bad_word = r.content\n",
        "    #bad_word=folder_path+\"NLP Toxicity/badwords.txt\"\n",
        "    data = pd.read_csv(BytesIO(bad_word), sep=\"\\n\", header=None)\n",
        "    bad_words = []\n",
        "    for i in data[0]:\n",
        "        bad_words.append(i)\n",
        "    return bad_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUld0q82LxVC"
      },
      "source": [
        "def create_tfids(bad_words,inference):\n",
        "    tfidfVec = TfidfVectorizer(max_features=1)\n",
        "    tfidf_inference = tfidfVec.fit_transform(inference)\n",
        "    tf_idf_bad_words = tfidfVec.fit_transform(bad_words)\n",
        "    return (tf_idf_bad_words,tfidf_inference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo8yrd9tLzQN"
      },
      "source": [
        "def calculate_bad_word_count(corpus,data):\n",
        "  # from sklearn.metrics.pairwise import cosine_similarity\n",
        "  from scipy import spatial\n",
        "  similarity_score = []\n",
        "  for i in range(len(data.toarray())):\n",
        "      similarity_score.append(1 - spatial.distance.cosine(corpus[0].toarray(), data[i].toarray()))\n",
        "  return similarity_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxwVh-HF30Sa"
      },
      "source": [
        "def name_entity(corpus):\n",
        "  ner = []\n",
        "  for index, row in corpus.iterrows():\n",
        "      valid_labels = ['PERSON','NORP','ORG','GPE','LANGUAGE']\n",
        "      if index % 5 == 0:\n",
        "          print(index,\"/\",corpus.shape[0])\n",
        "      doc = nlp(row['cleaned_text'])\n",
        "      labels = [(X.label_) for X in doc.ents]\n",
        "      if len(set(labels).intersection(set(valid_labels))) > 0:\n",
        "        ner.append(1)\n",
        "      else:\n",
        "        ner.append(0)\n",
        "  return ner\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxtn5Isb42BA"
      },
      "source": [
        "from scipy import sparse\n",
        "\n",
        "def get_features(headlines,vectors):\n",
        "    x_features =  headlines[['vader', 'afin',\n",
        "          'anger', 'anticipation','disgust', 'fear', 'joy', 'negative',\n",
        "          'positive', 'sadness','surprise','bad_word_score','NER_entity_count']]\n",
        "    vectors = sparse.csr_matrix(vectors)\n",
        "    x_inference = sparse.hstack([x_features,vectors]).tocsr()\n",
        "    return x_inference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzwj_a7rQM2j"
      },
      "source": [
        "def calculate(row):\n",
        "  row['toxic_score'] =(row['toxic_prob'] *.5 + .2* row['identity_hate_prob'] + .23* row['insult_prob'] + .33* row['obscene_prob'] + .15* row['severe_toxic_prob'] + 0.35*row['threat_prob'])/6\n",
        "  return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27JJqXCG7fgv"
      },
      "source": [
        "def predictToxicity(dataset,inference):\n",
        "    toxic_res = toxic.predict_proba(inference)\n",
        "    insult_res = insult.predict_proba(inference)\n",
        "    identity_hate_res = identity_hate.predict_proba(inference)\n",
        "    obscene_res = obscene.predict_proba(inference)\n",
        "    severe_toxic_res = severe_toxic.predict_proba(inference)\n",
        "    threat_res = threat.predict_proba(inference)\n",
        "    dataset['toxic_prob'] = toxic_res[:,1]\n",
        "    dataset['insult_prob'] = insult_res[:,1]\n",
        "    dataset['identity_hate_prob'] = identity_hate_res[:,1]\n",
        "    dataset['obscene_prob'] = obscene_res[:,1]\n",
        "    dataset['severe_toxic_prob'] = severe_toxic_res[:,1]\n",
        "    dataset['threat_prob'] = threat_res[:,1]\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xch8Q3L3Mx7t"
      },
      "source": [
        "def toxicityInference(data):\n",
        "  dataset=pd.DataFrame({ 'headline': data })\n",
        "  dataset['cleaned_text'] = dataset['headline']\n",
        "  #dataset['cleaned_text'] = dataset['headline'].apply(lambda x: clean_text(x))\n",
        "  tfidfVec = TfidfVectorizer(max_features=1)\n",
        "  tfidf_inference = tfidfVec.fit_transform(dataset['cleaned_text'])\n",
        "  dataset['anger'] = [0]\n",
        "  dataset['anticipation'] = [0]\n",
        "  dataset['disgust'] = [0]\n",
        "  dataset['fear'] = [0]\n",
        "  dataset['joy'] = [0]\n",
        "  dataset['negative'] = [0]\n",
        "  dataset['positive'] = [0]\n",
        "  dataset['sadness'] = [0]\n",
        "  dataset['surprise'] = [0]\n",
        "  dataset = dataset.apply(lambda x: add_sentiment(x),axis=1)\n",
        "  dataset = dataset.apply(lambda x: add_emotion(x),axis=1)\n",
        "  bad_words = create_bad_word_corpus()\n",
        "  tfid_bad_words,tfid_inference = create_tfids(bad_words,dataset['cleaned_text'])\n",
        "  dataset['bad_word_score'] = calculate_bad_word_count(tfid_bad_words,tfid_inference)\n",
        "  dataset['NER_entity_count'] = name_entity_recognition(dataset)\n",
        "  dataset = dataset.fillna(0.0,axis=1)\n",
        "  \n",
        "  return dataset,tfidf_inference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZMGbnq_2Ehw"
      },
      "source": [
        "def getBTSToxicityPrediction(X_body):\n",
        "  prob = 0\n",
        "  if X_body.size == 1: \n",
        "    #model1 https://drive.google.com/file/d/1-12STpKUEx0HKU39MyGGFkZTyhVMXQzV/view?usp=sharing\n",
        "    file_id = '1-12STpKUEx0HKU39MyGGFkZTyhVMXQzV'\n",
        "    model_filename = 'toxic.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    toxic = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "    #model 2  https://drive.google.com/file/d/1-3hQck_9SYBTM4V5yQLj-5mEwsVU--8s/view?usp=sharing\n",
        "    file_id = '1-3hQck_9SYBTM4V5yQLj-5mEwsVU--8s'\n",
        "    model_filename = 'insult.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    insult = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "    #model3 https://drive.google.com/file/d/1-3V--iK6yotcKceI-xJ-ndbJ2f_YBjCQ/view?usp=sharing\n",
        "    file_id = '1-3V--iK6yotcKceI-xJ-ndbJ2f_YBjCQ'\n",
        "    model_filename = 'identity_hate.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    identity_hate = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "    #model4 https://drive.google.com/file/d/1-86MpVy1_nre3NsBfGs1leeeO1heiFFn/view?usp=sharing\n",
        "    file_id = '1-86MpVy1_nre3NsBfGs1leeeO1heiFFn'\n",
        "    model_filename = 'obscene.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    obscene = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "    #model 5 https://drive.google.com/file/d/1-2WXq87JORCFFc26rJw8ViE0bBpquB27/view?usp=sharing\n",
        "    file_id = '1-2WXq87JORCFFc26rJw8ViE0bBpquB27'\n",
        "    model_filename = 'severe_toxic.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    severe_toxic = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "    #model 6 https://drive.google.com/file/d/1-62aInzttkgXMTRZnN7jR5o-L1ZyUzzB/view?usp=sharing\n",
        "    file_id = '1-62aInzttkgXMTRZnN7jR5o-L1ZyUzzB'\n",
        "    model_filename = 'threat.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    threat = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "    #model 7 https://drive.google.com/file/d/1-AVARlWjlPjHDNcgDKuxPEO2cZzklNQW/view?usp=sharing\n",
        "    file_id = '1-AVARlWjlPjHDNcgDKuxPEO2cZzklNQW'\n",
        "    model_filename = 'toxtruthOmeter.pickle'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    truthoMeter = pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "\n",
        "    dataset,f_matrix_infer = toxicityInference(X_body)\n",
        "    inference = get_features(dataset,f_matrix_infer)\n",
        "    #result = predictToxicity(dataset, inference)\n",
        "    #models\n",
        "    toxic_res = toxic.predict_proba(inference)\n",
        "    insult_res = insult.predict_proba(inference)\n",
        "    identity_hate_res = identity_hate.predict_proba(inference)\n",
        "    obscene_res = obscene.predict_proba(inference)\n",
        "    severe_toxic_res = severe_toxic.predict_proba(inference)\n",
        "    threat_res = threat.predict_proba(inference)\n",
        "    dataset['toxic_prob'] = toxic_res[:,1]\n",
        "    dataset['insult_prob'] = insult_res[:,1]\n",
        "    dataset['identity_hate_prob'] = identity_hate_res[:,1]\n",
        "    dataset['obscene_prob'] = obscene_res[:,1]\n",
        "    dataset['severe_toxic_prob'] = severe_toxic_res[:,1]\n",
        "    dataset['threat_prob'] = threat_res[:,1]\n",
        "    result = dataset\n",
        "    final_data = result[['headline','toxic_prob','insult_prob','identity_hate_prob','obscene_prob','severe_toxic_prob','threat_prob']]\n",
        "    final_data = final_data.apply(lambda x: calculate(x), axis=1)\n",
        "    pred = truthoMeter.predict_proba(final_data[['toxic_prob','insult_prob','identity_hate_prob','obscene_prob','severe_toxic_prob','threat_prob','toxic_score']])\n",
        "    predDf = pd.DataFrame(data=pred)\n",
        "    prob = predDf.max(axis=1)\n",
        "    return prob[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wWiPMDNwu4Z"
      },
      "source": [
        "\n",
        "##2.m. Define BTS Intent factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuVy0AIswufE"
      },
      "source": [
        "\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "def sentiment_score(data1):\n",
        "  \n",
        "  data1['scores'] = data1['preproces_stmt'].apply(lambda preproces_stmt: sid.polarity_scores(preproces_stmt))\n",
        "  data1['sentiment_score']=data1['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "  \n",
        "  return data1\n",
        "def get_word_tokens(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if len(token) > 1:\n",
        "            result.append(token)\n",
        "    return result\n",
        "\n",
        "def get_dictionary_print_words(data_final,colname):\n",
        "    tokenized_docs_local = data_final['preproces_stmt'].map(get_word_tokens)\n",
        "    processed_docs = data_final['preproces_stmt'].map(lambda doc: doc.split(\" \"))\n",
        "    dictionary_gensim = gensim.corpora.Dictionary(processed_docs)\n",
        "    \n",
        "    # count = 0\n",
        "    # # print('######## DICTIONARY Words and occurences ########')\n",
        "    # for k, v in dictionary_gensim.iteritems():\n",
        "    #     print(k, v)\n",
        "    #     count += 1\n",
        "    #     if count > 10:\n",
        "    #         break\n",
        "   \n",
        "    \n",
        "    return dictionary_gensim, tokenized_docs_local\n",
        "\n",
        "    \n",
        "def get_bow_corpus_print_sample(data_final,colname):\n",
        "  \n",
        "    dictionary_gensim, tokenized_docs_local = get_dictionary_print_words(data_final, colname)\n",
        "    # print(\"tokenized local\",tokenized_docs_local)\n",
        "    bow_corpus_local = [dictionary_gensim.doc2bow(doc) for doc in tokenized_docs_local]\n",
        "    bow_doc_local_0 = bow_corpus_local[0]\n",
        "    # print('\\n ######## BOW VECTOR FIRST ITEM ########')\n",
        "    # print(bow_corpus_local)\n",
        "    # print(bow_doc_local_0)\n",
        "    # print('\\n ######## PREVIEW BOW ########')\n",
        "    # for i in range(len(bow_doc_local_0)):\n",
        "        # print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_local_0[i][0], \n",
        "        #                                        dictionary_gensim[bow_doc_local_0[i][0]], bow_doc_local_0[i][1]))\n",
        "    return bow_corpus_local, dictionary_gensim\n",
        "def get_tfidf_corpus_print_sample(bow_corpus_local):\n",
        "    from gensim import corpora, models\n",
        "    tfidf = models.TfidfModel(bow_corpus_local)\n",
        "    tfidf_corpus_local = tfidf[bow_corpus_local]\n",
        "    # print('\\n ######## TFIDF VECTOR FIRST ITEM ########')\n",
        "    \n",
        "    # from pprint import pprint\n",
        "    # for doc in tfidf_corpus_local:\n",
        "    #     pprint(doc)\n",
        "    #     break\n",
        "    return tfidf_corpus_local\n",
        "def get_lda_model_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda, passes=2, workers=2)\n",
        "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=5,formatted=False)\n",
        "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
        "\n",
        "    #Below Code Prints Topics and Words\n",
        "    # for topic,words in lda_topics_words:\n",
        "    #     print(str(topic)+ \"::\"+ str(words))\n",
        "    return lda_model\n",
        "def get_lda_model_topics_topwords_print_top_topics(bow_corpusforlda,numtopics,dictionaryforlda):\n",
        "    lda_model = gensim.models.LdaMulticore(bow_corpusforlda, num_topics=numtopics, id2word=dictionaryforlda , random_state=1)\n",
        "    lda_all_topics=lda_model.show_topics(num_topics=numtopics, num_words=5,formatted=False)\n",
        "    lda_topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in lda_all_topics]\n",
        "\n",
        "    #Below Code Prints Topics and Words\n",
        "    # for topic,words in lda_topics_words:\n",
        "    #     print(str(topic)+ \"::\"+ str(words))\n",
        "    return lda_model,lda_topics_words\n",
        "def identify_topic_number_score_label_topwords(text,dictionary_local,lda_model_local,lda_topics_top_words_local):\n",
        "    bow_vector_local = dictionary_local.doc2bow(get_word_tokens(text))\n",
        "    topic_number_local, topic_score_local = sorted(\n",
        "        lda_model_local[bow_vector_local], key=lambda tup: -1*tup[1])[0]\n",
        "    #print (topic_number_local, topic_score_local)\n",
        "    return pd.Series([topic_number_local, topic_score_local,\" \".join(lda_topics_top_words_local[int(topic_number_local)][1])])\n",
        "def update_lda_results_to_dataset(dataframe,topiccolnames,coltoapplylda,colnamedictionary,colnameldamodel, colnameldatopwords):\n",
        "    dataframe[topiccolnames] = dataframe.apply(\n",
        "    lambda row: identify_topic_number_score_label_topwords(\n",
        "        row[coltoapplylda],colnamedictionary,colnameldamodel,\n",
        "        colnameldatopwords), axis=1)\n",
        "    return dataframe\n",
        "def topic_modelling(data_final): \n",
        "  bow_corpus_headline, dictionary_headline = get_bow_corpus_print_sample(data_final,\n",
        "                                                                      'preproces_stmt')\n",
        "  lda_model_headline, lda_headline_topic_words = get_lda_model_topics_topwords_print_top_topics(bow_corpus_headline, 2,dictionary_headline)\n",
        "  tfidf_corpus_headline = get_tfidf_corpus_print_sample(bow_corpus_headline)\n",
        "  lda_tfidf_model_headline  = get_lda_model_print_top_topics(tfidf_corpus_headline,10,dictionary_headline)\n",
        "  headlinetopiccolnames = ['topic_number','lda_score','topic_top_words']\n",
        "  data_x = update_lda_results_to_dataset(data_final, headlinetopiccolnames,'preproces_stmt', dictionary_headline, lda_model_headline, lda_headline_topic_words) \n",
        "  return data_x\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer=TfidfVectorizer(max_features=11, lowercase=False, ngram_range=(1,2))\n",
        "def prediction_func(model,x_test_x_vec):\n",
        "  yhat=model.predict_proba(x_test_x_vec)\n",
        "  pred=[]\n",
        "  for i in range(len(yhat)):\n",
        "    pred.append(np.max(yhat[i]))\n",
        "  pred=np.asarray(pred)\n",
        " \n",
        "  return pred\n",
        "\n",
        "def inference_intent(dataset,model):\n",
        "  headlines=dataset\n",
        "  \n",
        "  inf=pd.DataFrame({ 'preproces_stmt': headlines })\n",
        "  \n",
        "  \n",
        "  inf=sentiment_score(inf)\n",
        " \n",
        "  inference_data=topic_modelling(inf)\n",
        "  \n",
        "  vectorizer=TfidfVectorizer(max_features=1, lowercase=False, ngram_range=(1,2))\n",
        "  vec_post=vectorizer.fit_transform(inference_data['preproces_stmt'])\n",
        "\n",
        "  data_post=inference_data.drop(columns=['preproces_stmt','scores','topic_top_words','topic_number'])\n",
        "  \n",
        "  data_post_vec = sparse.hstack([data_post, vec_post]).tocsr()\n",
        "  score=prediction_func(model,data_post_vec)\n",
        "  \n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83QpPEh81NYu"
      },
      "source": [
        "def getDistilledIntentPrediction(X_news):\n",
        "  prob = 0\n",
        "  if X_news.size == 1: \n",
        "    file_id = '1-2kd2LMXSrmwu5zKiZnqqHYPq9pAAcVP'\n",
        "    model_filename = 'intent_retrained.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename) \n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best__intent_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    prob=inference_intent(X_news,best__intent_model)\n",
        "    # prob = best__intent_model.predict_proba(X_news)[:,1]\n",
        "  return float(prob)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmJ1qqH8mIwM"
      },
      "source": [
        "## Ninjas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8i-CTRemKGc"
      },
      "source": [
        "##2.n. Social Credibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dWNWCAbmNOP"
      },
      "source": [
        "#Social credibility Microfactor\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "import nltk.sentiment\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import warnings\n",
        "import seaborn as sb\n",
        "from pprint import pprint\n",
        "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "import pickle\n",
        "\n",
        "def preprocess_text(document):    \n",
        "    news = re.sub(\"[^a-zA-Z]\", \" \", document)    \n",
        "    news =  news.lower()    \n",
        "    news_words = nltk.word_tokenize( news)    \n",
        "    stops = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    words = [w for w in  news_words  if not w in stops]    \n",
        "    wordnet_lem = [ WordNetLemmatizer().lemmatize(w) for w in words ]    \n",
        "    stems = [nltk.stem.SnowballStemmer('english').stem(w) for w in wordnet_lem ]    \n",
        "    return \" \".join(stems)\n",
        "\n",
        "def Polarity(snt):\n",
        "    if not snt:\n",
        "        return None\n",
        "    elif snt['neg'] > snt['pos'] and snt['neg'] > snt['neu']:\n",
        "        return -1\n",
        "    elif snt['pos'] > snt['neg'] and snt['pos'] > snt['neu']:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def TypePolarity(sentence):\n",
        "    sentimentVector = []\n",
        "    senti = nltk.sentiment.vader.SentimentIntensityAnalyzer()\n",
        "    snt = senti.polarity_scores(sentence)\n",
        "    sentimentVector.append(Polarity(snt))\n",
        "    sentimentVector.append(snt['neg'])\n",
        "    sentimentVector.append(snt['neu'])\n",
        "    sentimentVector.append(snt['pos'])\n",
        "    sentimentVector.append(snt['compound'])\n",
        "    \n",
        "    return sentimentVector\n",
        "\n",
        "def get_sentiment(df,column):\n",
        "  df['processed'] = df[column].apply(preprocess_text) \n",
        "  sentiment = []\n",
        "  vader_pol = []\n",
        "  cmp_score = []\n",
        "  for row in df['processed']:\n",
        "      get_pols = TypePolarity(row)\n",
        "      sentiment.append(get_pols[1:])\n",
        "      vader_pol.append(get_pols[0])\n",
        "      cmp_score.append(get_pols[1:][-1])\n",
        "    \n",
        "  df['vader_polarity'] = vader_pol\n",
        "  df['sentiment_score'] = cmp_score \n",
        "  df = df[['vader_polarity','sentiment_score']]\n",
        "  return df       \n",
        "\n",
        "#Best pickled model for sentiment analysis\n",
        "\n",
        "def getSocialScore(d):\n",
        "\n",
        "  # extracting feature analysis features for model\n",
        "  df2 = extract_features_stream(d)\n",
        "  column='tweet'\n",
        "\n",
        "  trained_df2 = get_sentiment(df2,column)\n",
        "  #sentiment_model ='/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Shreya/Week 9/SentimentModel.pkl'\n",
        "  file_id='1uRnwjCnP79PKwvXnE9eAuZHabfYHVPjX'\n",
        "  model_filename = 'SentimentModel.pkl'\n",
        "  downloaded = gdrive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(model_filename)\n",
        "  sentiment_model = '/content/{}'.format(model_filename)\n",
        "  loaded_model = pickle.load(open(sentiment_model, 'rb'))\n",
        "  predicted_sentiment = loaded_model.predict(trained_df2)\n",
        "  predicted_scores = loaded_model.predict_proba(trained_df2)\n",
        "\n",
        "  df2.loc[:,'sentiment'] = predicted_sentiment\n",
        "  df2.loc[:,'senti_0'] = predicted_scores[:,0]\n",
        "  df2.loc[:,'senti_1'] = predicted_scores[:,1]\n",
        "  df2 = df2[[column,'sentiment','senti_0','senti_1','user_mentions','retweet_count','favorite_count','hashtags_count','tweet_id']]\n",
        "\n",
        "  # print(\"inside content modelling\")\n",
        "  trained_df3 = df2[['user_mentions','retweet_count','favorite_count','hashtags_count']]\n",
        "  #content_model = '/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Shreya/Week 9/Retweet_HashtagModel.pkl'\n",
        "  file_id='1ceLrZBnWJLCd0klxDTQIC9Z_K7q5cCK0'\n",
        "  model_filename = 'Retweet_HashtagModel.pkl'\n",
        "  downloaded = gdrive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(model_filename)\n",
        "  content_model = '/content/{}'.format(model_filename)\n",
        "  loaded_model = pickle.load(open(content_model, 'rb'))\n",
        "  predicted_content = loaded_model.predict(trained_df3)\n",
        "  df2.loc[:,'content'] = predicted_content\n",
        "\n",
        "  # print(df2.columns)\n",
        "\n",
        "  #result_df = df2[[column,'sentiment','senti_0','senti_1','topic_0','topic_1','content']]\n",
        "  result_df = df2[[column,'senti_0','content','tweet_id']]\n",
        "  getScore(result_df)\n",
        "  return getScore(result_df)\n",
        "\n",
        "def getScore(result_df):\n",
        "  result_df['content'] = result_df['content'].apply(pd.to_numeric)      \n",
        "  result_df['social_credibility'] = result_df['senti_0']*0.5 + result_df['content']*0.3\n",
        "  result_df = result_df[['tweet_id','tweet','social_credibility']]\n",
        "  mean_score=result_df[\"social_credibility\"].mean()\n",
        "  # print(mean_score)\n",
        "  return mean_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8uSzywRmOrg"
      },
      "source": [
        "##2.o. Content Veracity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOEhYecUmSAK"
      },
      "source": [
        "import os, json, errno\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sys import argv\n",
        "import string\n",
        "import time\n",
        "from multiprocessing import Process\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import re\n",
        "from nltk.corpus import stopwords as stp\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "def to_unix_tmsp(col):\n",
        "    return pd.DatetimeIndex(col).astype(np.int64) / 1e6\n",
        "\n",
        "def parse_twitter_datetime(timestr):\n",
        "    return pd.datetime.strptime(timestr, \"%a %b %d %H:%M:%S %z %Y\")\n",
        "class Tweets:\n",
        "\n",
        "    def __init__(self, event_name, output_dir=\"data/tweets\"):\n",
        "        self.event = event_name\n",
        "        self.data = {}\n",
        "        self.output_dir = output_dir\n",
        "        self.printable = set(string.printable)\n",
        "    \n",
        "    def append(self, twt, cat, thrd, is_src):\n",
        "        twt['category'] = cat\n",
        "        twt[\"thread\"] = thrd\n",
        "        twt[\"event\"] = self.event\n",
        "        twt[\"is_src\"] = is_src\n",
        "\n",
        "        twt_text=twt[\"text\"]\n",
        "        twt_text_filtered=str()\n",
        "        for c in twt_text:\n",
        "            if c in self.printable:\n",
        "                twt_text_filtered+=c\n",
        "        text_features=self.tweettext2features(twt_text_filtered)\n",
        "        has_question = \"?\" in twt[\"text\"]\n",
        "        has_exclaim = \"!\" in twt[\"text\"]\n",
        "        twt['twt_text_filtered']=twt_text_filtered\n",
        "\n",
        "        features = {\n",
        "            \"is_rumour\": lambda obj : 1 if obj['category'] == \"rumours\" else 0,\n",
        "            \"tweet_id\" : lambda obj : obj.get(\"id\"),\n",
        "            \"tweet\": lambda obj: obj.get('text'),\n",
        "            \"filtered_tweet\": lambda obj: obj.get('twt_text_filtered'),\n",
        "            \"event\" : lambda obj : obj.get(\"event\"),\n",
        "            \"thread\" : lambda obj : obj[\"thread\"],\n",
        "            #structural features\n",
        "            \"tweet_length\": lambda obj : len(obj.get(\"text\",\"\")),\n",
        "            \"symbol_count\": lambda obj: len(obj[\"entities\"].get(\"symbols\", [])),\n",
        "            \"user_mentions\": lambda obj: len(obj[\"entities\"].get(\"user_mentions\", [])),\n",
        "            \"urls_count\": lambda obj : len(obj[\"entities\"].get(\"urls\", [])),\n",
        "            \"media_count\": lambda obj: len(obj[\"entities\"].get(\"media\", [])),\n",
        "            \"hashtags_count\": lambda obj : len(obj[\"entities\"].get(\"hashtags\", [])),\n",
        "            \"retweet_count\": lambda obj : obj.get(\"retweet_count\", 0),\n",
        "            \"favorite_count\": lambda obj : obj.get(\"favorite_count\"),\n",
        "            \"mentions_count\": lambda obj : len(obj[\"entities\"].get(\"user_mentions\", \"\")),\n",
        "            \"has_smile_emoji\": lambda obj: 1 if \"😊\" in obj[\"text\"] else 0,\n",
        "            \"has_place\": lambda obj: 1 if obj.get(\"place\") else 0,\n",
        "            \"has_coords\": lambda obj: 1 if obj.get(\"coordinates\") else 0,\n",
        "            \"has_quest\": lambda obj: 1 if has_question else 0,\n",
        "            \"has_exclaim\": lambda obj: 1 if has_exclaim else 0,\n",
        "            \"has_quest_or_exclaim\": lambda obj: 1 if (has_question or has_exclaim) else 0,\n",
        "            \"sensitive\": lambda obj: 1 if obj.get(\"possibly_sensitive\") else 0,            \n",
        "\n",
        "        }\n",
        "\n",
        "        for col in features:\n",
        "            self.data.setdefault(col, []).append(features[col](twt))\n",
        "\n",
        "        for col in text_features:\n",
        "            self.data.setdefault(col, []).append(text_features[col])\n",
        "\n",
        "    def tweettext2features(self, tweet_text):   \n",
        "        def punctuationanalysis(tweet_text):\n",
        "            punctuations= [\"\\\"\",\"(\",\")\",\"*\",\",\",\"-\",\"_\",\".\",\"~\",\"%\",\"^\",\"&\",\"!\",\"#\",'@'\n",
        "               \"=\",\"\\'\",\"\\\\\",\"+\",\"/\",\":\",\"[\",\"]\",\"«\",\"»\",\"،\",\"؛\",\"?\",\".\",\"…\",\"$\",\n",
        "               \"|\",\"{\",\"}\",\"٫\",\";\",\">\",\"<\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]\n",
        "            hasperiod=sum(c =='.' for c in tweet_text)\n",
        "            number_punct=sum(c in punctuations for c in tweet_text)\n",
        "            return {'hasperiod':hasperiod,'number_punct':number_punct}\n",
        "\n",
        "        def negativewordcount(tokens):\n",
        "            count = 0\n",
        "            negativeFeel = ['tired', 'sick', 'bord', 'uninterested', 'nervous', 'stressed',\n",
        "                            'afraid', 'scared', 'frightened', 'boring','bad',\n",
        "                            'distress', 'uneasy', 'angry', 'annoyed', 'pissed',\"hate\",\n",
        "                            'sad', 'bitter', 'down', 'depressed', 'unhappy','heartbroken','jealous', 'fake', 'stupid', 'strange','absurd', 'crazy']\n",
        "            for negative in negativeFeel:\n",
        "                if negative in tokens:\n",
        "                    count += 1\n",
        "            return count\n",
        "\n",
        "        def positivewordcount(tokens):\n",
        "            count = 0\n",
        "            positivewords = ['joy', ' happy', 'hope', 'kind', 'surprise'\n",
        "                            , 'excite', ' interest', 'admire',\"delight\",\"yummy\",\n",
        "                            'confidenc', 'good', 'satisf', 'pleasant',\n",
        "                            'proud', 'amus', 'amazing', 'awesome',\"love\",\"passion\",\"great\",\"like\",\"wow\",\"delicious\", \"true\", \"correct\", \"crazy\"]\n",
        "            for pos in positivewords:\n",
        "                if pos in tokens:\n",
        "                    count += 1\n",
        "            return count\n",
        "\n",
        "        def capitalratio(tweet_text):\n",
        "            uppers = [l for l in tweet_text if l.isupper()]\n",
        "            capitalratio = len(uppers) / len(tweet_text)\n",
        "            return capitalratio\n",
        "\n",
        "        def contentlength(words):\n",
        "            wordcount = len(words)\n",
        "            return wordcount\n",
        "\n",
        "        def sentimentscore(tweet_text):\n",
        "            analysis = TextBlob(tweet_text)\n",
        "            return analysis.sentiment.polarity\n",
        "\n",
        "        def getposcount(tweet_text):\n",
        "            postag = []\n",
        "            poscount = {}\n",
        "            poscount['Noun']=0\n",
        "            poscount['Verb']=0\n",
        "            poscount['Adjective'] = 0\n",
        "            poscount['Pronoun']=0\n",
        "            poscount['FirstPersonPronoun']=0\n",
        "            poscount['SecondPersonPronoun']=0\n",
        "            poscount['ThirdPersonPronoun']=0\n",
        "            poscount['Adverb']=0\n",
        "            Nouns = {'NN','NNS','NNP','NNPS'}\n",
        "            Verbs={'VB','VBP','VBZ','VBN','VBG','VBD','To'}\n",
        "            first_person_pronouns=['I','me','my','mine','we','us','our','ours']\n",
        "            second_person_pronouns=['you','your','yours']\n",
        "            third_person_pronouns=['he','she','it','him','her','it','his','hers','its','they','them','their','theirs']\n",
        "\n",
        "            word_tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+', '', tweet_text))\n",
        "            for word in word_tokens:\n",
        "                w_lower=word.lower()\n",
        "                if w_lower in first_person_pronouns:\n",
        "                    poscount['FirstPersonPronoun']+=1\n",
        "                elif w_lower in second_person_pronouns:\n",
        "                    poscount['SecondPersonPronoun']+=1\n",
        "                elif w_lower in third_person_pronouns:\n",
        "                    poscount['ThirdPersonPronoun']+=1\n",
        "\n",
        "            postag = nltk.pos_tag(word_tokens)\n",
        "            for g1 in postag:\n",
        "                if g1[1] in Nouns:\n",
        "                    poscount['Noun'] += 1\n",
        "                elif g1[1] in Verbs:\n",
        "                    poscount['Verb']+= 1\n",
        "                elif g1[1]=='ADJ'or g1[1]=='JJ':\n",
        "                    poscount['Adjective']+=1\n",
        "                elif g1[1]=='PRP' or g1[1]=='PRON':\n",
        "                    poscount['Pronoun']+=1\n",
        "                elif g1[1]=='ADV':\n",
        "                    poscount['Adverb']+=1\n",
        "            return poscount\n",
        "        def tweets2tokens(tweet_text):\n",
        "            tokens = nltk.word_tokenize(re.sub(r'([^\\s\\w]|_)+','', tweet_text.lower()))\n",
        "            url=0\n",
        "            for token in tokens:\n",
        "                if token.startswith( 'http' ):\n",
        "                    url=1\n",
        "\n",
        "            return tokens,url\n",
        "\n",
        "\n",
        "        # the code for def tweettext2features(tweet_text):\n",
        "        features=dict()\n",
        "\n",
        "        tokens,url=tweets2tokens(tweet_text)\n",
        "\n",
        "        punc_dict=punctuationanalysis(tweet_text)\n",
        "        features.update(punc_dict)\n",
        "        features['negativewordcount']=(negativewordcount(tokens))\n",
        "        features['positivewordcount']=(positivewordcount(tokens))\n",
        "        features['capitalratio']=(capitalratio(tweet_text))\n",
        "        features['contentlength']=(contentlength(tokens))\n",
        "        features['sentimentscore']=(sentimentscore(tweet_text))\n",
        "        pos_dict=getposcount(tweet_text)\n",
        "        features.update(pos_dict)\n",
        "        features['has_url_in_text']=(url)\n",
        "        #print(\"features\",features)\n",
        "        return features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrjmluDzmVWy"
      },
      "source": [
        "#function to scrape data from the twitter\n",
        "def scrape(keywords):\n",
        "  import tweepy\n",
        "  from tweepy import OAuthHandler\n",
        "  import json\n",
        "  from tweepy import Stream\n",
        "  from tweepy.streaming import StreamListener\n",
        "  import csv\n",
        "  import re\n",
        "  consumer_key ='iz8iwNx5S829k1HHZNiKW7FjR'\n",
        "  consumer_secret = 'MBm1XBlcMuKiZNpaow3n6pD6JlsebhSUFymBwPK4IrNWumbeKQ'\n",
        "  access_token = \"1308414905888972800-nNxvZKdxWvik4sCIDY7xxpTkmnxygd\"\n",
        "  access_secret = \"3hDFrHeB4xNTjOwq4OYdps4RBJQOiYftDxuAKImo6bVcD\"\n",
        "  auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_secret)\n",
        "  api = tweepy.API(auth)\n",
        "  list_of_strings  = [i.text for i in keywords]\n",
        "  class MyListener(StreamListener):\n",
        "    count=0\n",
        "\n",
        "    def on_data(self, data):\n",
        "        try:\n",
        "            if MyListener.count>3:\n",
        "               return False\n",
        "            MyListener.count=MyListener.count+1\n",
        "            with open('python.json', 'a') as f:\n",
        "                f.write(data)\n",
        "                return True\n",
        "        except BaseException as e:\n",
        "            print(\"Error on_data: %s\" % str(e))\n",
        "        return True\n",
        " \n",
        "    def on_error(self, status):\n",
        "        print(status)\n",
        "        return True\n",
        "\n",
        "  twitter_stream = Stream(auth, MyListener())\n",
        "  tweets=twitter_stream.filter(track=list_of_strings)\n",
        "  return tweets\n",
        "\n",
        "\n",
        "\n",
        "#to get keywords from the sentence using rake_nltk\n",
        "!pip install rake_nltk\n",
        "from rake_nltk import Rake\n",
        "def get_keywords(text):\n",
        "  rake_nltk_var = Rake()\n",
        "  rake_nltk_var.extract_keywords_from_text(text)\n",
        "  keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
        "  print(keyword_extracted)\n",
        "  return keyword_extracted;\n",
        "\n",
        "\n",
        "!pip3 install -U spacy\n",
        "!python3 -m spacy download en_core_web_lg\n",
        "\n",
        "\n",
        "#to get keywords from the sentence using spacy\n",
        "def get_keywords_spacy(text):\n",
        "  import spacy\n",
        "  nlp = spacy.load('en_core_web_lg')\n",
        "  doc = nlp(text)\n",
        "  return doc.ents\n",
        "\n",
        "#function to determine content veracity score\n",
        "def contentvearcity_score(t):\n",
        "  import pickle\n",
        "  d = extract_features_stream(t)\n",
        "  temp=d\n",
        "  d=d[['tweet_length','symbol_count','urls_count',\n",
        "                 'media_count','has_smile_emoji', 'hashtags_count', 'retweet_count','has_place', 'has_coords',\n",
        "                 'has_quest_or_exclaim','number_punct','negativewordcount',\n",
        "                 'positivewordcount', 'capitalratio','contentlength', 'sentimentscore', \n",
        "                 'Noun', 'Verb', 'Adjective','Pronoun', 'FirstPersonPronoun', \n",
        "                 'SecondPersonPronoun','ThirdPersonPronoun', 'Adverb']]\n",
        "          \n",
        "  # load_rf=pickle.load(open('/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Neural Net.pkl','rb'))\n",
        "  file_id = '1v6YfxMg3SDTBsm7HU8qkeEetQeKHhLl_'\n",
        "  model_filename = 'Neural Net.pkl'\n",
        "  downloaded = gdrive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(model_filename)\n",
        "  pickle_filepath = '/content/{}'.format(model_filename)\n",
        "  load_rf = pickle.load(open(pickle_filepath, 'rb'))\n",
        "  predict_rf=load_rf.predict(d)\n",
        "  scores_rf = load_rf.predict_proba(d)\n",
        " \n",
        "  d.loc[:,'contentVeracityScore'] = scores_rf[:,0]\n",
        "  d.loc[:,'tweet_id'] = temp['tweet_id']\n",
        "  d = d[['tweet_id','contentVeracityScore']]\n",
        "  mean_score=d[\"contentVeracityScore\"].mean()\n",
        "  return d,mean_score\n",
        "\n",
        "#function to extract features from the tweets\n",
        "def extract_features_stream(tweets):\n",
        "  import pandas as pd\n",
        "  import json\n",
        "  df = pd.read_json (r'/content/python.json',lines=True)  \n",
        "  df.to_csv (r'/content/tweet.csv', index = None)\n",
        "  df=pd.read_csv('/content/tweet.csv')\n",
        "\n",
        "\n",
        "  #print(df.columns)\n",
        "  d=pd.DataFrame(columns=['tweet_id','tweet','tweet_length','symbol_count','urls_count','media_count','retweet_count','favorite_count','hashtags_count','user_mentions','has_smile_emoji','has_place','has_coords','has_quest','has_exclaim','has_quest_or_exclaim','sensitive','hasperiod','number_punct', 'negativewordcount', 'positivewordcount', 'capitalratio', 'contentlength', 'sentimentscore', 'Noun', 'Verb', 'Adjective', 'Pronoun', 'FirstPersonPronoun' ,'SecondPersonPronoun' ,'ThirdPersonPronoun', 'Adverb', 'Sentiment'])\n",
        "  for i in df.index:\n",
        "    has_question = \"?\" in df[\"text\"][i]\n",
        "    has_exclaim = \"!\" in df[\"text\"][i]\n",
        "    #print(df)\n",
        "    has_quest_or_exclaim=(has_question or has_exclaim)\n",
        "    #print(type(df.entities[i]))\n",
        "    df.entities[i] = df.entities[i].replace(\"\\'\", \"\\\"\")\n",
        "    en=json.loads(df.entities[i])\n",
        "    #print(df['possibly_sensitive'][i])\n",
        "    #df['possibly_sensitive'][i].replace(np.nan,0)\n",
        "    features={\n",
        "        'tweet_id': df['id'][i],\n",
        "        'tweet': df['text'][i],\n",
        "        'tweet_length':len(df['text'][i]),\n",
        "        'symbol_count':len(en['symbols']),\n",
        "        'urls_count':len(en['urls']),\n",
        "        'retweet_count':df['retweet_count'][i],\n",
        "        'favorite_count':df['favorite_count'][i],\n",
        "        'media_count': len(en['media']) if 'media' in en else 0,\n",
        "        'user_mentions':len(en['user_mentions']),\n",
        "        'hashtags_count':len(en['hashtags']),\n",
        "        'has_smile_emoji':1 if \"😊\" in df[\"text\"][i] else 0,\n",
        "        'has_place':1 if df[\"place\"][i] else 0,\n",
        "        'has_coords':1 if df[\"coordinates\"][i] else 0,\n",
        "        'has_quest':has_question,\n",
        "        'has_exclaim':has_exclaim,\n",
        "        'has_quest_or_exclaim':has_quest_or_exclaim,\n",
        "        #'sensitive':1 if df[\"possibly_sensitive\"][i] else 0,\n",
        "    }\n",
        "    text_features=Tweets.tweettext2features(tweets,df['text'][i])\n",
        "    #print(text_features)\n",
        "    for col in features:\n",
        "            d.loc[i,col]=features[col]\n",
        "    \n",
        "    for col in text_features:\n",
        "      #print(col,text_features[col])\n",
        "      d.loc[i,col]=text_features[col]\n",
        "  \n",
        "  d.to_csv('stream_features.csv', sep='\\t')\n",
        "  #contentvearcity_score(d)\n",
        "  \n",
        "  return d\n",
        "\n",
        "#Pipeline for getting content veracity score for streamed data\n",
        "!pip install rake_nltk\n",
        "def Testing(df,column): \n",
        "  scores=[]\n",
        "  !rm python.json\n",
        "  !rm stream_features.csv\n",
        "  !rm tweet.csv\n",
        "  keywords=get_keywords_spacy(df[column].to_string())\n",
        "  tweets=scrape(keywords)\n",
        "  score = merge(tweets)\n",
        "  scores.append(score)\n",
        "  !rm python.json\n",
        "  !rm stream_features.csv\n",
        "  !rm tweet.csv\n",
        "  return scores[0]\n",
        "\n",
        "def merge(tweets):\n",
        "    List,contentVeracity = contentvearcity_score(tweets)\n",
        "    socialCrebility = getSocialScore(tweets)\n",
        "    accuracy = [0.5624, 0.8756, 0.1]\n",
        "    w = [float(i)/sum(accuracy) for i in accuracy]\n",
        "    score = contentVeracity*w[0] + socialCrebility*w[1] \n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBfWgxMQmYFw"
      },
      "source": [
        "##2.p. News Coverage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmOpcQnVmZK1"
      },
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "import nltk.sentiment\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn import metrics\n",
        "from sklearn import svm\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import warnings\n",
        "import seaborn as sb\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from string import punctuation\n",
        "import collections\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import en_core_web_sm\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "      \n",
        "!pip install vaderSentiment      \n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "def scrape(keywords):\n",
        "  import tweepy\n",
        "  from tweepy import OAuthHandler\n",
        "  import json\n",
        "  from tweepy import Stream\n",
        "  from tweepy.streaming import StreamListener\n",
        "  import csv\n",
        "  import re\n",
        "  consumer_key ='iz8iwNx5S829k1HHZNiKW7FjR'\n",
        "  consumer_secret = 'MBm1XBlcMuKiZNpaow3n6pD6JlsebhSUFymBwPK4IrNWumbeKQ'\n",
        "  access_token = \"1308414905888972800-nNxvZKdxWvik4sCIDY7xxpTkmnxygd\"\n",
        "  access_secret = \"3hDFrHeB4xNTjOwq4OYdps4RBJQOiYftDxuAKImo6bVcD\"\n",
        "  auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_secret)\n",
        "  api = tweepy.API(auth)\n",
        "  list_of_strings  = [i.text for i in keywords]\n",
        "  class MyListener(StreamListener):\n",
        "    count=0\n",
        "\n",
        "    def on_data(self, data):\n",
        "        try:\n",
        "            if MyListener.count>3:\n",
        "               return False\n",
        "            MyListener.count=MyListener.count+1\n",
        "            with open('python.json', 'a') as f:\n",
        "                f.write(data)\n",
        "                return True\n",
        "        except BaseException as e:\n",
        "            print(\"Error on_data: %s\" % str(e))\n",
        "        return True\n",
        " \n",
        "    def on_error(self, status):\n",
        "        print(status)\n",
        "        return True\n",
        "\n",
        "  twitter_stream = Stream(auth, MyListener())\n",
        "  tweets=twitter_stream.filter(track=list_of_strings)\n",
        "  return tweets\n",
        "\n",
        "#function to extract features from the tweets\n",
        "def extract_features_stream(tweets):\n",
        "  import pandas as pd\n",
        "  import json\n",
        "  df = pd.read_json (r'/content/python.json',lines=True)  \n",
        "  df.to_csv (r'/content/tweet.csv', index = None)\n",
        "  df=pd.read_csv('/content/tweet.csv')\n",
        "\n",
        "  d=pd.DataFrame(columns=['tweet_id','tweet','tweet_length','symbol_count','urls_count','media_count','retweet_count','favorite_count','hashtags_count','user_mentions','has_smile_emoji','has_place','has_coords','has_quest','has_exclaim','has_quest_or_exclaim','sensitive','hasperiod','number_punct', 'negativewordcount', 'positivewordcount', 'capitalratio', 'contentlength', 'sentimentscore', 'Noun', 'Verb', 'Adjective', 'Pronoun', 'FirstPersonPronoun' ,'SecondPersonPronoun' ,'ThirdPersonPronoun', 'Adverb', 'Sentiment'])\n",
        "  for i in df.index:\n",
        "    has_question = \"?\" in df[\"text\"][i]\n",
        "    has_exclaim = \"!\" in df[\"text\"][i]\n",
        "    has_quest_or_exclaim=(has_question or has_exclaim)\n",
        "    df.entities[i] = df.entities[i].replace(\"\\'\", \"\\\"\")\n",
        "    en=json.loads(df.entities[i])\n",
        "    features={\n",
        "        'tweet_id': df['id'][i],\n",
        "        'tweet': df['text'][i],\n",
        "        'tweet_length':len(df['text'][i]),\n",
        "        'symbol_count':len(en['symbols']),\n",
        "        'urls_count':len(en['urls']),\n",
        "        'retweet_count':df['retweet_count'][i],\n",
        "        'favorite_count':df['favorite_count'][i],\n",
        "        'media_count': len(en['media']) if 'media' in en else 0,\n",
        "        'user_mentions':len(en['user_mentions']),\n",
        "        'hashtags_count':len(en['hashtags']),\n",
        "        'has_smile_emoji':1 if \"😊\" in df[\"text\"][i] else 0,\n",
        "        'has_place':1 if df[\"place\"][i] else 0,\n",
        "        'has_coords':1 if df[\"coordinates\"][i] else 0,\n",
        "        'has_quest':has_question,\n",
        "        'has_exclaim':has_exclaim,\n",
        "        'has_quest_or_exclaim':has_quest_or_exclaim,\n",
        "    }\n",
        "    text_features=Tweets.tweettext2features(tweets,df['text'][i])\n",
        "    for col in features:\n",
        "            d.loc[i,col]=features[col]\n",
        "    \n",
        "    for col in text_features:\n",
        "      d.loc[i,col]=text_features[col]\n",
        "  \n",
        "  d.to_csv('stream_features.csv', sep='\\t')\n",
        "  \n",
        "  return d\n",
        "\n",
        "def clean_text(d, text_field):\n",
        "      d[text_field] = d[text_field].str.lower()\n",
        "      d[text_field] = d[text_field].apply(lambda elem: re.sub('([#])|([/])|([\\])|([:])|([~])|([.])|([_])|([[]])|([]])|([^a-zA-Z])',' ',   elem)) \n",
        "      d = clean_text(d, 'tweet')\n",
        "      nlp = en_core_web_sm.load() \n",
        "      tokenizer = RegexpTokenizer(r'\\w+')\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      stop = set(stopwords.words('english'))\n",
        "      punctuation = list(string.punctuation) #already taken care of with the cleaning function.\n",
        "      stop.update(punctuation)\n",
        "      w_tokenizer = WhitespaceTokenizer()\n",
        "      return d\n",
        "\n",
        "\n",
        "def get_text_sentiment(text):\n",
        "  analyzer = SentimentIntensityAnalyzer()\n",
        "  _sentiment = analyzer.polarity_scores(text)[\"compound\"]\n",
        "  return 'Positive' if abs(_sentiment) > 0.5 else 'Negative'\n",
        "\n",
        "def NewsCoverage_score(d):\n",
        "    import pickle\n",
        "    d = extract_features_stream(d)\n",
        "    temp=d\n",
        "    d['Sentiment'] = d['tweet'].apply(get_text_sentiment)\n",
        "    d=d[['tweet', 'tweet_length', 'retweet_count', 'hashtags_count', 'negativewordcount', 'positivewordcount',\n",
        "       'sentimentscore', 'Sentiment']]\n",
        "    print(d)  \n",
        "    d['tweet'] = LabelEncoder().fit_transform(d['tweet'].astype('str')) \n",
        "    d['Sentiment'] = LabelEncoder().fit_transform(d['Sentiment'].astype('str'))       \n",
        "    #load_rf=pickle.load(open('/content/drive/MyDrive/MLSpring-2021/teams/ninjas/NLP Project/Tripura/newscoverage.pkl','rb'))\n",
        "    file_id = '1DdYhS7YGqVKffH3T0g36zwtgEXQCaKK7'\n",
        "    model_filename = 'NewsCoverage.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    load_rf = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    predict_rf=load_rf.predict(d)\n",
        "    scores_rf = load_rf.predict_proba(d)\n",
        "    d.loc[:,'NewsCoverage'] = scores_rf[:,0]\n",
        "    d.loc[:,'tweet_id'] = temp['tweet_id']\n",
        "    #d.loc[:,'tweet'] = temp['tweet']\n",
        "    d = d[['tweet_id','NewsCoverage']]\n",
        "    mean_score=d[\"NewsCoverage\"].mean()\n",
        "    return d, mean_score\n",
        "\n",
        "    #Pipeline for gettingnews coverage score for streamed data\n",
        "# !pip install rake_nltk\n",
        "# def Testing(df,column): \n",
        "#   import pandas as pd\n",
        "#   scores_list=[]\n",
        "#   scores=[]\n",
        "#   !rm python.json\n",
        "#   !rm stream_features.csv\n",
        "#   !rm tweet.csv\n",
        "#   keywords=get_keywords_spacy(df[column][0])\n",
        "#   tweets=scrape(keywords) \n",
        "#   score = merge(tweets)\n",
        "#   !rm python.json\n",
        "#   !rm stream_features.csv\n",
        "#   !rm tweet.csv\n",
        "\n",
        "#   return score\n",
        "\n",
        "# def merge(tweets):\n",
        "#     List,contentVeracity = contentvearcity_score(tweets)\n",
        "#     socialCrebility = getSocialScore(tweets)\n",
        "#     List,newscoverage = NewsCoverage_score(tweets)\n",
        "\n",
        "#     accuracy = [0.5624, 0.8756, 0.1]\n",
        "#     w = [float(i)/sum(accuracy) for i in accuracy]\n",
        "\n",
        "#     score = contentVeracity*w[0] + socialCrebility*w[1] + newscoverage*w[2]\n",
        "#     return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDdWQYszvZLR"
      },
      "source": [
        "\n",
        "##2.q. Define Musketeers Writing Style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muWDW1nKvgmZ"
      },
      "source": [
        "# Utilities\n",
        "from nltk import word_tokenize\n",
        "import math\n",
        "import collections as coll\n",
        "\n",
        "# Functions to estimate features for Lexical Analysis\n",
        "def strip(str):\n",
        "    return str.strip().split('\\t')[0]\n",
        "\n",
        "def count_characters(str):\n",
        "    return len(str)\n",
        "\n",
        "def count_words(str):\n",
        "    word = str.split()\n",
        "    return len(word)\n",
        "\n",
        "def avg_word_length(str):\n",
        "    words = str.split()\n",
        "    word_lengths = [len(word) for word in words]\n",
        "    avg_word_length = sum(word_lengths)/len(words)\n",
        "    return avg_word_length\n",
        "\n",
        "def hash_tag(str):\n",
        "    words = str.split()\n",
        "    hashtags = [word for word in words if word.startswith('#')]\n",
        "    return len(hashtags)\n",
        "\n",
        "# NORMALIZED OVER LENGTH OF CHUNK\n",
        "    \n",
        "def special_char_count(text):\n",
        "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
        "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
        "    count = 0\n",
        "    for i in text:\n",
        "        if (i in st):\n",
        "            count = count + 1\n",
        "    return count/len(text)\n",
        "\n",
        "def count_puncuation(text):\n",
        "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
        "    count = 0\n",
        "    for i in text:\n",
        "        if (i in st):\n",
        "            count = count + 1\n",
        "    return float(count) / float(len(text))\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "def RemoveSpecialCHs(text):\n",
        "    text = word_tokenize(text)\n",
        "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
        "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
        "\n",
        "    words = [word for word in text if word not in st]\n",
        "    return words\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# also returns Honore Measure R\n",
        "def hapaxLegemena(text):\n",
        "    words = RemoveSpecialCHs(text)\n",
        "    V1 = 0\n",
        "    # dictionary comprehension . har word kay against value 0 kardi\n",
        "    freqs = {key: 0 for key in words}\n",
        "    for word in words:\n",
        "        freqs[word] += 1\n",
        "    for word in freqs:\n",
        "        if freqs[word] == 1:\n",
        "            V1 += 1\n",
        "    N = len(words)\n",
        "    V = float(len(set(words)))\n",
        "    R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
        "    h = V1 / N\n",
        "    return R, h\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def hapaxDisLegemena(text):\n",
        "    words = RemoveSpecialCHs(text)\n",
        "    count = 0\n",
        "    # Collections as coll Counter takes an iterable collapse duplicate and counts as\n",
        "    # a dictionary how many equivelant items has been entered\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words)\n",
        "    for word in freqs:\n",
        "        if freqs[word] == 2:\n",
        "            count += 1\n",
        "\n",
        "    h = count / float(len(words))\n",
        "    S = count / float(len(set(words)))\n",
        "    return S, h\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1 - (sigma(n(n - 1))/N(N-1)\n",
        "# N is total number of words\n",
        "# n is the number of each type of word\n",
        "def SimpsonsIndex(text):\n",
        "    words = RemoveSpecialCHs(text)\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words)\n",
        "    N = len(words)\n",
        "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
        "    D = 1 - (n / (N * (N - 1)))\n",
        "    return D\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# -1*sigma(pi*lnpi)\n",
        "# Shannon and sympsons index are basically diversity indices for any community\n",
        "def ShannonEntropy(text):\n",
        "    words = RemoveSpecialCHs(text)\n",
        "    lenght = len(words)\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words)\n",
        "    arr = np.array(list(freqs.values()))\n",
        "    distribution = 1. * arr\n",
        "    distribution /= max(1, lenght)\n",
        "    import scipy as sc\n",
        "    H = sc.stats.entropy(distribution, base=2)\n",
        "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
        "    return H\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# K  10,000 * (M - N) / N**2\n",
        "# , where M  Sigma i**2 * Vi.\n",
        "def YulesCharacteristicK(text):\n",
        "    words = RemoveSpecialCHs(text)\n",
        "    N = len(words)\n",
        "    freqs = coll.Counter()\n",
        "    freqs.update(words)\n",
        "    vi = coll.Counter()\n",
        "    vi.update(freqs.values())\n",
        "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
        "    K = 10000 * (M - N) / math.pow(N, 2)\n",
        "    return K\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# logW = V-a/log(N)\n",
        "# N = total words , V = vocabulary richness (unique words) ,  a=0.17\n",
        "# we can convert into log because we are only comparing different texts\n",
        "def BrunetsMeasureW(text):\n",
        "    words = RemoveSpecialCHs(text)\n",
        "    a = 0.17\n",
        "    V = float(len(set(words)))\n",
        "    N = len(words)\n",
        "    B = (V - a) / (math.log(N))\n",
        "    return B\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
        "def typeTokenRatio(text):\n",
        "    words = word_tokenize(text)\n",
        "    return len(set(words)) / len(words)\n",
        "\n",
        "################# Lexical Utility  Functions #################\n",
        "\n",
        "def data_preprocess_lexical(sentence):\n",
        "  arr = []\n",
        "\n",
        "  # Stripping the \\n that appears in each sentence\n",
        "  data = strip(sentence)\n",
        "\n",
        "  # Char Count\n",
        "  arr.append(len(sentence))\n",
        "\n",
        "  # Word count\n",
        "  arr.append(len(sentence.split()))\n",
        "\n",
        "  # Counting the total number of characters\n",
        "  arr.append(count_characters(data))\n",
        "\n",
        "  # Counting the total number of words\n",
        "  arr.append(count_words(data))\n",
        "\n",
        "  # Calculating the average word length\n",
        "  arr.append(avg_word_length(data))\n",
        "\n",
        "  # Calculating the number of hash tags in the sentence\n",
        "  arr.append(hash_tag(data))\n",
        "\n",
        "  # Calculating the number of special characters in the sentence\n",
        "  arr.append(special_char_count(data))\n",
        "\n",
        "  # Calculating the puncuations from the sentence\n",
        "  arr.append(count_puncuation(data))\n",
        "\n",
        "  return arr\n",
        "\n",
        "################# Readibility Utility  Functions #################\n",
        "def data_preprocess_readibility(sentence):\n",
        "  arr = []\n",
        "\n",
        "  arr.append(len(sentence))\n",
        "  arr.append(len(sentence.split()))\n",
        "  arr.append(textstat.flesch_reading_ease(sentence))\n",
        "  arr.append(textstat.flesch_kincaid_grade(sentence))\n",
        "  arr.append(textstat.coleman_liau_index(sentence))\n",
        "  return arr\n",
        "\n",
        "################# Vocab Utility  Functions #################\n",
        "def data_preprocess_vocab(sentence):\n",
        "  arr = []\n",
        "\n",
        "  # Calculate ShannonEntropy\n",
        "  arr.append(ShannonEntropy(sentence))\n",
        "\n",
        "  # Calculate YulesCharacteristicK\n",
        "  arr.append(YulesCharacteristicK(sentence))\n",
        "\n",
        "  # Calculate BrunetsMeasureW\n",
        "  arr.append(BrunetsMeasureW(sentence))\n",
        "\n",
        "  # Calculate typeTokenRatio\n",
        "  arr.append(typeTokenRatio(sentence))\n",
        "\n",
        "  return arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-4OODbMvmM8"
      },
      "source": [
        "import joblib\n",
        "lexical_model = joblib.load('/content/drive/MyDrive/MLSpring-2021/MLSpring-2021/TeamIntegration_MLSpring2021/models/Shiv - Writing Analysis/classifier_lexical__AdaBoost')\n",
        "readibility_model = joblib.load('/content/drive/MyDrive/MLSpring-2021/MLSpring-2021/TeamIntegration_MLSpring2021/models/Shiv - Writing Analysis/classifier_readibility_AdaBoost')\n",
        "vocab_model = joblib.load('/content/drive/MyDrive/MLSpring-2021/MLSpring-2021/TeamIntegration_MLSpring2021/models/Shiv - Writing Analysis/classifier_vocab_AdaBoost')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrfAauAPvpOx"
      },
      "source": [
        "def writing_style_score(text):\n",
        "  print(text[0])\n",
        "  accuracy = [0.75, 0.71, 0.72]\n",
        "  w = [float(i)/sum(accuracy) for i in accuracy]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  if ( (text != \"\")):\n",
        "      prob.append(w[0] * lexical_model.predict_proba([data_preprocess_lexical(text)])[0])\n",
        "      sumW += w[0]\n",
        "      prob.append(w[1] * vocab_model.predict_proba([data_preprocess_readibility(text)])[0])\n",
        "      sumW += w[1]\n",
        "      prob.append(w[2] * readibility_model.predict_proba([data_preprocess_vocab(text)])[0])\n",
        "      sumW += w[2]\n",
        "  probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "  return probTotal[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsP_N5IL0Z7k"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6lB2xt6K6p8"
      },
      "source": [
        "## 2.r Define Political Affiliation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAEVLBsI4wap"
      },
      "source": [
        "class PartyAffiliation():\n",
        "    \n",
        "    # API to check whether the subject(Headline) is present in the \n",
        "    # - democrats most used words if the party affiliation is democrat\n",
        "    # - republicans most used words if the part affiliation is republican\n",
        "\n",
        "    file_id = '1A_GLWEEYYG-SgLsRHa4h3wvYW95y0Kz5'\n",
        "    model_filename = 'countDemV'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    countDemV = '/content/{}'.format(model_filename)\n",
        "\n",
        "    file_id = '1bPq4UO1VhWzV1JpHIcbvKxX8UIYxZYDx'\n",
        "    model_filename = 'countRepV'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    countRepV = '/content/{}'.format(model_filename)\n",
        "\n",
        "    file_id = '1bkzyVNhkLxfDq6eDPA4YAW24-raWGu_Z'\n",
        "    model_filename = 'Party_affiliation_model'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    Party_affiliation_model = '/content/{}'.format(model_filename)\n",
        "\n",
        "\n",
        "    def partyAffiliationFromHeadline(self, r):\n",
        "        v = r['subject_str']\n",
        "        p = r['party_str']\n",
        "        if (p =='democrat'):\n",
        "            s2 = set(self.countDemV.get_feature_names())\n",
        "        if (p =='republican'):\n",
        "            s2 = set(self.countRepV.get_feature_names())\n",
        "        if (p != 'democract' and p !='republican'):\n",
        "            return 1 #'true'        \n",
        "        if set(v).intersection(s2):\n",
        "            return 1 #'true'\n",
        "        else:\n",
        "            return 0 #'false'\n",
        "\n",
        "    #API to convert true, mostly-true and half-true to true\n",
        "    # false, barely-true and pants-fire to false\n",
        "    def convertMulticlassToBinaryclass(self, r):\n",
        "        v = r['label']\n",
        "        if (v == 'true'):\n",
        "            return 1 #'true'\n",
        "        if (v == 'mostly-true'):\n",
        "            return 1 #'true'\n",
        "        if (v == 'half-true'):\n",
        "            return 1 #'true'\n",
        "        if (v == 'barely-true'):\n",
        "            return 0 #'false'\n",
        "        if (v == 'false'):\n",
        "            return 0 #'false'\n",
        "        if (v == 'pants-fire'):\n",
        "            return 0 #'false'\n",
        "            \n",
        "    def predict(self, headline, party):\n",
        "                \n",
        "        #creating the dataframe with our text so we can leverage the existing code\n",
        "        dfrme = pd.DataFrame(index=[0], columns=['subject', 'party_str'])\n",
        "        dfrme['subject_str'] = headline\n",
        "        dfrme['party_str'] = party        \n",
        "\n",
        "        dfrme['subject'] = headline\n",
        "        dfrme['subject_str'] = dfrme['subject'].astype(str).str.split() \n",
        "        dfrme['label_str'] = dfrme.apply(self.partyAffiliationFromHeadline, axis=1)\n",
        "        \n",
        "        x = dfrme['label_str'].values.reshape(-1, 1)\n",
        "        predicted = self.party_model.predict(x)\n",
        "        predicedProb = self.party_model.predict_proba(x)[:,1]\n",
        "        return predicted, predicedProb\n",
        "\n",
        "    def __init__(self): \n",
        "\n",
        "        self.countDemV = load('countDemV') \n",
        "        self.countRepV = load('countRepV') \n",
        "        self.party_model = load('Party_affiliation_model') \n",
        "\n",
        "f = PartyAffiliation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwV5c4mM6vSY"
      },
      "source": [
        "partyAffiliation = PartyAffiliation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOSOnOWx6oSi"
      },
      "source": [
        "def getPartyAffiliationScore(headline, partyName): # return between 0 and 1, being 0 = True,  1 = Fake\n",
        "    if ( (headline == \"\") | (partyName == \"\") ):\n",
        "        return 0\n",
        "    binaryValue, probValue = partyAffiliation.predict(headline, partyName)\n",
        "    return (1 - float(probValue))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk7cLNuXHIu4"
      },
      "source": [
        "Political Affiliation - Sentiment Analysis Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKyAJwuNHKlD"
      },
      "source": [
        "class VaderAnalysis():\n",
        "\n",
        "    def __init__(self):   \n",
        "      \n",
        "        columnNames = [\"jsonid\", \"label\", \"headline_text\", \"subject\", \"speaker\", \"speaker_job_title\", \"state_info\", \"party_affiliation\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\",\"clean\", \"sentiment_vector\",\"vader_polarity\", \"sentiment_score\"]\n",
        "        \n",
        "        file_id = '1-EKrGBL7FvoJb2H2yX4QXTvVUENjV4x-'\n",
        "        model_filename = 'test_sentiment.csv'\n",
        "        downloaded = gdrive.CreateFile({'id': file_id})\n",
        "        downloaded.GetContentFile(model_filename)\n",
        "        pickle_filepath = '/content/{}'.format(model_filename)\n",
        "        dataTest = pd.read_csv(pickle_filepath, sep=',', header=None, names = columnNames)\n",
        "\n",
        "        file_id = '1EJYm_MOQsKjhLRCaWBAb5PUL68WRk6zh'\n",
        "        model_filename = 'train_sentiment.csv'\n",
        "        downloaded = gdrive.CreateFile({'id': file_id})\n",
        "        downloaded.GetContentFile(model_filename)\n",
        "        pickle_filepath = '/content/{}'.format(model_filename)\n",
        "        dataTrain = pd.read_csv(pickle_filepath, sep=',', header=None, names = columnNames)\n",
        "\n",
        "        #dropping columns\n",
        "        columnsToRemove = ['jsonid', 'subject', 'speaker','speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context', 'sentiment_vector']\n",
        "        dataTrain = dataTrain.drop(columns=columnsToRemove)\n",
        "        dataTest = dataTest.drop(columns=columnsToRemove)\n",
        "        dataTrain = dataTrain.loc[1:] \n",
        "        dataTest = dataTest.loc[1:]\n",
        "    \n",
        "    \n",
        "        tfidfV = TfidfVectorizer(stop_words='english', min_df=5, max_df=30, use_idf=True, smooth_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
        "\n",
        "        self.logR_pipeline = Pipeline([\n",
        "                ('LogRCV', tfidfV),\n",
        "                ('LogR_clf',LogisticRegression(solver='liblinear', C=32/100))\n",
        "                ])\n",
        "\n",
        "        self.logR_pipeline = self.logR_pipeline.fit(dataTrain['headline_text'],dataTrain['vader_polarity'])\n",
        "\n",
        "        predicted_LogR = self.logR_pipeline.predict(dataTest['headline_text'])\n",
        "        score = metrics.accuracy_score(dataTest['vader_polarity'], predicted_LogR)\n",
        "        \n",
        "        \n",
        "    def predict(self, text):\n",
        "        predicted = self.logR_pipeline.predict([text])\n",
        "        predicedProb = self.logR_pipeline.predict_proba([text])[:,1]\n",
        "        return float(predicedProb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGu658q7HQh6"
      },
      "source": [
        "va = VaderAnalysis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ugmYM3tHZFS"
      },
      "source": [
        "Political Affiliation - ClickBait Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czpe6UO2l1uE"
      },
      "source": [
        "def getClickbaitScore(X_news):\n",
        "  prob = 0\n",
        "  if X_news.size == 1:\n",
        "    file_id = '176VDZ587yjypLa1jT9qftTk-uq0GdC2c'\n",
        "    model_filename = 'ClickBait_Model.pkl'\n",
        "    downloaded = gdrive.CreateFile({'id': file_id})\n",
        "    downloaded.GetContentFile(model_filename)\n",
        "    pickle_filepath = '/content/{}'.format(model_filename)\n",
        "    best_distilled_clickbait_model = pickle.load(open(pickle_filepath, 'rb'))\n",
        "    prob = best_distilled_clickbait_model.predict_proba(X_news)[:,1]\n",
        "  return float(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J40Er33rywt0"
      },
      "source": [
        "Generate Political Affiliation Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5_Ej5Os0nUz"
      },
      "source": [
        "def generatePoliticalAffiliationScore(text , news):\n",
        "  accuracy = [0.9, 0.95, 0.53]\n",
        "  w = [float(i)/sum(accuracy) for i in accuracy]\n",
        "  sumW = 0\n",
        "  prob = []\n",
        "  prob.append(w[0] * getClickbaitScore(news))\n",
        "  sumW += w[0]\n",
        "  prob.append(w[1] * va.predict(text))\n",
        "  sumW += w[1]\n",
        "  prob.append(w[2] * getPartyAffiliationScore(text,'none'))\n",
        "  sumW += w[2]\n",
        "  probTotal = sum(prob[0:len(prob)]) / sumW\n",
        "  return probTotal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8O68pYBvak0"
      },
      "source": [
        "# Data Divers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX_gHXKyve2E"
      },
      "source": [
        "## Malicious account factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U124XQQcvkNJ"
      },
      "source": [
        "from io import StringIO\n",
        "\n",
        "def read_from_drive(url):\n",
        "  file_id = url.split('/')[-2]\n",
        "  dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
        "  url = requests.get(dwn_url).content\n",
        "  raw = BytesIO(url)\n",
        "  return raw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYq2F_YLvlpJ"
      },
      "source": [
        "import textstat\n",
        "\n",
        "def get_textstat(text):\n",
        "  data = {'flesch_reading_ease':[textstat.flesch_reading_ease(text)],\n",
        "          'smog_index':[textstat.smog_index(text)],\n",
        "          'flesch_kincaid_grade':[textstat.flesch_kincaid_grade(text)],\n",
        "          'coleman_liau_index':[textstat.coleman_liau_index(text)],\n",
        "          'automated_readability_index':[textstat.automated_readability_index(text)],\n",
        "          'dale_chall_readability_score':[textstat.dale_chall_readability_score(text)],\n",
        "          'difficult_words':[textstat.difficult_words(text)],\n",
        "          'linsear_write_formula':[textstat.linsear_write_formula(text)],\n",
        "          'gunning_fog':[textstat.gunning_fog(text)],\n",
        "          'text_standard':[textstat.text_standard(text)],\n",
        "          'fernandez_huerta':[textstat.fernandez_huerta(text)]\n",
        "          }\n",
        "  return pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jHDlbFqvneD"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.spatial import distance\n",
        "import statistics\n",
        "\n",
        "def cosine_sim(list):\n",
        "  if len(list) < 1:\n",
        "    return 0\n",
        "    \n",
        "  sims = []\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "  matrix = tfidf_vectorizer.fit_transform(list)\n",
        "  matrix = matrix.toarray()\n",
        "  for i in range(0, matrix.shape[0]-1):\n",
        "      vect1 = matrix[i]\n",
        "      vect2 = matrix[i+1]\n",
        "      sims.append(distance.cosine(vect1, vect2))\n",
        "\n",
        "  if len(sims) > 0:\n",
        "    return statistics.mean(sims)\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ6cthHKCHgs"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdfIZu2zvo_r"
      },
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def sentiment_scores(sentence):\n",
        "    sid_obj = SentimentIntensityAnalyzer()\n",
        "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
        "    return sentiment_dict['compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EfK5Ljivqbz"
      },
      "source": [
        "def y_true(stream_data):\n",
        "  target_levels = []\n",
        "  target_levels_dict = {'true': 0, 'mostly-true': 1, 'half-true': 2, 'barely-true': 3, 'mostly-false': 3, 'false': 4, 'pants-fire': 5}\n",
        "  for target in stream_data['target']:\n",
        "    target_levels.append(target_levels_dict[target])\n",
        "  return target_levels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NhbO3nivr3r"
      },
      "source": [
        "import _pickle as cPickle\n",
        "\n",
        "m1_data = read_from_drive('https://drive.google.com/file/d/1-2zjFaaxznewERyTjj2QlKr_YvlklPls/view?usp=sharing')\n",
        "base_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "m2_data = read_from_drive('https://drive.google.com/file/d/1u7hSP6S7C1CgG_mpiSQbvL8TspHldKDp/view?usp=sharing')\n",
        "cosine_model = cPickle.loads(m2_data.getvalue())\n",
        "\n",
        "m3_data = read_from_drive('https://drive.google.com/file/d/1TPCW6Yf4XFRC1LQjs7tzC8ffzuRKQKAh/view?usp=sharing')\n",
        "sentiment_model = cPickle.loads(m3_data.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puc1XrVZvtYB"
      },
      "source": [
        "def ensemble(newsList):\n",
        "  aggr_result = predict_feed(newsList, base_model, 0)*0.05 + predict_feed(newsList, cosine_model, 1)*0.9 + predict_feed(newsList, sentiment_model, 2)*0.05\n",
        "  # a = np.array(aggr_result) \n",
        "  # index = a.argmax()   \n",
        "  # return index\n",
        "  return aggr_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ5PGxcevu8W"
      },
      "source": [
        " def ma_prediction(data): \n",
        "  y_preds = np.zeros(shape=(data.shape[0], 6))\n",
        "  preds = data['preprocessed_body'].apply(ensemble)\n",
        "  for i in range(len(preds)):\n",
        "    y_preds[i,0] = preds.iloc[i][0][0]\n",
        "    y_preds[i,5] = preds.iloc[i][0][1]\n",
        "  return y_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsl8737vwct"
      },
      "source": [
        "def predict_feed(news, model, f):\n",
        "  \n",
        "  reals = [news, news]\n",
        "  sentiment = sentiment_scores(news)\n",
        "  cosine = cosine_sim(reals)\n",
        "\n",
        "  test_feature = get_textstat(news)\n",
        "  test_feature.drop(['text_standard'], axis=1, inplace=True)\n",
        "  if f == 1:\n",
        "    test_feature['cosine_sim'] = cosine\n",
        "  if f == 2:\n",
        "    test_feature['sentiment_compound'] = sentiment\n",
        "\n",
        "  return model.predict_proba(test_feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSsymfLcv4-G"
      },
      "source": [
        "## Credibility and Reliability Factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFo9IZlHv80R"
      },
      "source": [
        "def read_csv_from_drive(url):\n",
        "  file_id = url.split('/')[-2]\n",
        "  dwn_url='https://drive.google.com/uc?export=download&id=' + file_id\n",
        "  url = requests.get(dwn_url).text\n",
        "  csv_raw = StringIO(url)\n",
        "  return pd.read_csv(csv_raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DneiFLVRv_3x"
      },
      "source": [
        "def load_amalgamated_dataset():\n",
        "  return read_csv_from_drive(\"https://drive.google.com/file/d/1BXS7P19YLErlXqCrUpIRsFXkODhTyrnG/view?usp=sharing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkKGa68twDwe"
      },
      "source": [
        "def predict_mf_1_cred_of_author(tfidf_vectorizer, stream_test_dataset, politifact_amalgamated_dataset, df):\n",
        "  sparse_matrix_test = tfidf_vectorizer.transform(stream_test_dataset['preprocessed_statement_text'])\n",
        "  # OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
        "  doc_term_matrix_test = sparse_matrix_test.todense()\n",
        "  df_test = pd.DataFrame(doc_term_matrix_test, \n",
        "                      columns=tfidf_vectorizer.get_feature_names(), \n",
        "                      index=stream_test_dataset['preprocessed_statement_text'])\n",
        "  df_test\n",
        "  similarity_matrix = cosine_similarity(df_test, df)\n",
        "  true = []\n",
        "  mostly_true = []\n",
        "  barely_true = []\n",
        "  half_true = []\n",
        "  false = []\n",
        "  pants_fire = []\n",
        "  total_news = []\n",
        "  target_levels = []\n",
        "  credibility = []\n",
        "  for i in similarity_matrix:\n",
        "    #Got the highest similarity index for the article\n",
        "    maximum_val = i.argsort()[-1:][::-1]\n",
        "    true.append(politifact_amalgamated_dataset['true'][maximum_val[0]])\n",
        "    mostly_true.append(politifact_amalgamated_dataset['mostly_true'][maximum_val[0]])\n",
        "    barely_true.append(politifact_amalgamated_dataset['barely_true'][maximum_val[0]])\n",
        "    half_true.append(politifact_amalgamated_dataset['half_true'][maximum_val[0]])\n",
        "    false.append(politifact_amalgamated_dataset['false'][maximum_val[0]])\n",
        "    pants_fire.append(politifact_amalgamated_dataset['pants_fire'][maximum_val[0]])\n",
        "    total_news.append(politifact_amalgamated_dataset['total_news'][maximum_val[0]])\n",
        "    target_levels.append(politifact_amalgamated_dataset['target_level'][maximum_val[0]])\n",
        "    credibility.append(politifact_amalgamated_dataset['credibility'][maximum_val[0]])\n",
        "  stream_test_dataset['true'] = true\n",
        "  stream_test_dataset['mostly_true'] = mostly_true\n",
        "  stream_test_dataset['half_true'] = half_true\n",
        "  stream_test_dataset['barely_true'] = barely_true\n",
        "  stream_test_dataset['false'] = false\n",
        "  stream_test_dataset['pants_fire'] = pants_fire\n",
        "  stream_test_dataset['total_news'] = total_news\n",
        "  stream_test_dataset['target_level'] = target_levels\n",
        "  stream_test_dataset['credibility'] = credibility\n",
        "  return stream_test_dataset\n",
        "\n",
        "def predict_mf_2_content_quality(amalgamated_dataset, column_name):\n",
        "  flesh_scores = []\n",
        "  smog_indexes = []\n",
        "  flesch_kincaid_grades = []\n",
        "  coleman_liau_indexes = []\n",
        "  automated_readability_indexes = []\n",
        "  dale_chall_readability_scores = []\n",
        "  linsear_write_formulas = []\n",
        "  gunning_fogs = []\n",
        "  fernandez_huertas = []\n",
        "  for i in range(len(amalgamated_dataset)):\n",
        "    statement = amalgamated_dataset[column_name].iloc[i]\n",
        "\n",
        "    flesh_score = textstat.flesch_reading_ease(statement)\n",
        "    smog_index = textstat.smog_index(statement)\n",
        "    flesch_kincaid_grade = textstat.flesch_kincaid_grade(statement)\n",
        "    coleman_liau_index = textstat.coleman_liau_index(statement)\n",
        "    automated_readability_index = textstat.automated_readability_index(statement)\n",
        "    dale_chall_readability_score = textstat.dale_chall_readability_score(statement)\n",
        "    linsear_write_formula = textstat.linsear_write_formula(statement)\n",
        "    gunning_fog = textstat.gunning_fog(statement)\n",
        "    fernandez_huerta = textstat.fernandez_huerta(statement)\n",
        "\n",
        "\n",
        "\n",
        "    flesh_scores.append(flesh_score)\n",
        "    smog_indexes.append(smog_index)\n",
        "    flesch_kincaid_grades.append(flesch_kincaid_grade)\n",
        "    coleman_liau_indexes.append(coleman_liau_index)\n",
        "    automated_readability_indexes.append(automated_readability_index)\n",
        "    dale_chall_readability_scores.append(dale_chall_readability_score)\n",
        "    linsear_write_formulas.append(linsear_write_formula)\n",
        "    gunning_fogs.append(gunning_fog)\n",
        "    fernandez_huertas.append(fernandez_huerta)\n",
        "\n",
        "  amalgamated_dataset['flesh_scores'] = flesh_scores\n",
        "  amalgamated_dataset['smog_indexes'] = smog_indexes\n",
        "  amalgamated_dataset['flesch_kincaid_grades'] = flesch_kincaid_grades\n",
        "  amalgamated_dataset['coleman_liau_indexes'] = coleman_liau_indexes\n",
        "  amalgamated_dataset['automated_readability_indexes'] = automated_readability_indexes\n",
        "  amalgamated_dataset['dale_chall_readability_scores'] = dale_chall_readability_scores\n",
        "  amalgamated_dataset['linsear_write_formulas'] = linsear_write_formulas\n",
        "  amalgamated_dataset['gunning_fogs'] = gunning_fogs\n",
        "  amalgamated_dataset['fernandez_huertas'] = fernandez_huertas\n",
        "  return amalgamated_dataset\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "def get_sentences(text):\n",
        "    return re.split('\\? |, |!|\\n', text)\n",
        "\n",
        "def get_words(sentences):\n",
        "    return sum([sentence.split() for sentence in sentences], [])\n",
        "\n",
        "def get_average_word_length(words):\n",
        "    return sum([len(word) for word in words]) // len(words)\n",
        "  \n",
        "def get_average_word_based_sentence_length(sentences):\n",
        "    return sum([len(sentence.split()) for sentence in sentences]) // len(sentences)\n",
        "\n",
        "def get_average_chars_based_sentence_length(sentences):\n",
        "    return sum([len(sentence) for sentence in sentences]) // len(sentences)\n",
        "  \n",
        "def get_punctuations_count(text):\n",
        "    return len([c for c in text if c in string.punctuation])\n",
        "\n",
        "\n",
        "def predict_mf_3_subjectivity(dataset):\n",
        "  polarities = []\n",
        "  subjectivities = []\n",
        "  avg_word_len = []\n",
        "  avg_words_based_sen_len = []\n",
        "  avg_chars_based_sen_len = []\n",
        "  pun_count = []\n",
        "  for title in dataset['preprocessed_statement_text']:\n",
        "    polarity, subjectivity = TextBlob(title).sentiment\n",
        "    polarities.append(polarity)\n",
        "    subjectivities.append(subjectivity)\n",
        "    avg_word_len.append(get_average_word_length(title))\n",
        "    avg_words_based_sen_len.append(get_average_word_based_sentence_length(title))\n",
        "    avg_chars_based_sen_len.append(get_average_chars_based_sentence_length(title))\n",
        "    pun_count.append(get_punctuations_count(title))\n",
        "  new_dataset = dataset[['preprocessed_statement_text']]\n",
        "  new_dataset['polarities'] = polarities\n",
        "  new_dataset['subjectivities'] = subjectivities\n",
        "  new_dataset['avg_word_len'] = avg_word_len\n",
        "  new_dataset['avg_words_based_sen_len'] = avg_words_based_sen_len\n",
        "  new_dataset['avg_chars_based_sen_len'] = avg_chars_based_sen_len\n",
        "  new_dataset['pun_count'] = pun_count\n",
        "  return new_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiyB-jT-wBMR"
      },
      "source": [
        "def create_tf_on_training(politifact_amalgamated_dataset):\n",
        "\n",
        "  print(politifact_amalgamated_dataset.columns)\n",
        "  tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "  sparse_matrix = tfidf_vectorizer.fit_transform(politifact_amalgamated_dataset['statement'])\n",
        "\n",
        "  doc_term_matrix = sparse_matrix.todense()\n",
        "  df = pd.DataFrame(doc_term_matrix, \n",
        "                    columns=tfidf_vectorizer.get_feature_names(), \n",
        "                    index=politifact_amalgamated_dataset['statement'])\n",
        "  return tfidf_vectorizer, df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FM6OIEOwKRr"
      },
      "source": [
        "politifact_amalgamated_dataset = load_amalgamated_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIXO-T0PwIHY"
      },
      "source": [
        "target_levels = politifact_amalgamated_dataset['target_level'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIx_TkVAwLy5"
      },
      "source": [
        "def predict_credibility_and_reliability(newsFeed, model, f):\n",
        "  y_pred = []\n",
        "  if f == 1:\n",
        "    politifact_amalgamated_dataset = load_amalgamated_dataset()\n",
        "    tf_idf, df = create_tf_on_training(politifact_amalgamated_dataset)\n",
        "    stream_data = predict_mf_1_cred_of_author(tf_idf, newsFeed, politifact_amalgamated_dataset, df)\n",
        "    test_features = stream_data[['true', 'mostly_true','half_true','barely_true','false','pants_fire','total_news','credibility']]\n",
        "    # y_pred = model.predict_proba(test_features.values)\n",
        "    y_pred = model.predict_proba(test_features.values)\n",
        "  elif f == 2:\n",
        "    stream_data = predict_mf_2_content_quality(newsFeed, 'preprocessed_statement_text')\n",
        "    test_features = stream_data[['flesh_scores','smog_indexes','flesch_kincaid_grades','automated_readability_indexes','dale_chall_readability_scores','fernandez_huertas']]\n",
        "    # print(test_features.columns)\n",
        "    # y_pred = model.predict_proba(test_features)\n",
        "    y_pred = model.predict_proba(test_features)\n",
        "  elif f == 3:\n",
        "    stream_data = predict_mf_3_subjectivity(newsFeed)\n",
        "    test_features = stream_data.drop(['preprocessed_statement_text'],axis=1)\n",
        "    # y_pred = model.predict_proba(test_features.values)\n",
        "    y_pred = model.predict_proba(test_features.values)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrDrYL_3wNZ1"
      },
      "source": [
        "def f_credibility_and_relability_ensembling(stream_data):\n",
        "\n",
        "  m1_data = read_from_drive('https://drive.google.com/file/d/1-2QHkqBRcKSGl_4ivhE0R6PkvIvBJ3LU/view?usp=sharing')\n",
        "  cred_of_author_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "  m1_data = read_from_drive('https://drive.google.com/file/d/1-BNeYIrEE7hLj62uZ8cxSBg4F4609TFo/view?usp=sharing')\n",
        "  content_quality_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "  m1_data = read_from_drive('https://drive.google.com/file/d/1-03SrbJR4n6KBMaMwFIZeMsd8q2b9omB/view?usp=sharing')\n",
        "  subjectivity_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "\n",
        "  mf_y_pred_1 = predict_credibility_and_reliability(stream_data, cred_of_author_model, 1)\n",
        "  mf_y_pred_2 = predict_credibility_and_reliability(stream_data, content_quality_model, 2)\n",
        "  mf_y_pred_3 = predict_credibility_and_reliability(stream_data, subjectivity_model, 3)\n",
        "  #Weighted Ensembling Of MicroFactor Models\n",
        "  accuracy = [0.45, 0.24, 0.44]\n",
        "  weights = [accuracy[0]/sum(accuracy), accuracy[1]/sum(accuracy), accuracy[2]/sum(accuracy)]\n",
        "  final_pred = (mf_y_pred_1*weights[0] + mf_y_pred_2*weights[1] + mf_y_pred_3*weights[2])\n",
        "  return final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47_4kOT9wPMu"
      },
      "source": [
        "random_news_items = get_random_news_items(10)\n",
        "final_pred = f_credibility_and_relability_ensembling(random_news_items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x4HnFNBwRaH"
      },
      "source": [
        "## Long term utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTXaaOQewVZq"
      },
      "source": [
        "def dataset_preprocessing(data):\n",
        "  # news_author = news_author_dataset()\n",
        "  data = data[data['target'].isin(['true','mostly-true','half-true','barely-true', 'mostly-false','false','pants-fire'])]\n",
        "  target_class = []\n",
        "  target_class_dict = {'true': 0, 'mostly-true': 1, 'half-true': 2, 'barely-true': 3, 'mostly-false': 3, 'false': 4, 'pants-fire': 5}\n",
        "  for i in data['target']:\n",
        "    target_class.append(target_class_dict[i])\n",
        "  data['target_class'] = target_class\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Qyid6kpwXl7"
      },
      "source": [
        "def combine(news_author, news_source):\n",
        "  true = []\n",
        "  mostly_true = []\n",
        "  barely_true = []\n",
        "  half_true = []\n",
        "  false = []\n",
        "  pants_fire = []\n",
        "  total_news = []\n",
        "  credibility = []\n",
        "  for i in news_author['source']:\n",
        "    value = news_source.index[news_source['source'] == i].tolist()\n",
        "    if len(value) > 0:\n",
        "      value = value[0]\n",
        "      true.append(news_source['true'][value])\n",
        "      mostly_true.append(news_source['mostly_true'][value])\n",
        "      half_true.append(news_source['half_true'][value])\n",
        "      barely_true.append(news_source['mostly_false'][value])\n",
        "      false.append(news_source['false'][value])\n",
        "      pants_fire.append(news_source['pants_on_fire'][value])\n",
        "      total_news.append(news_source['total_news'][value])\n",
        "      credibility.append(news_source['credibility'][value])\n",
        "    else:\n",
        "      true.append(0)\n",
        "      mostly_true.append(0)\n",
        "      half_true.append(0)\n",
        "      barely_true.append(0)\n",
        "      false.append(0)\n",
        "      pants_fire.append(0)\n",
        "      total_news.append(0)\n",
        "      credibility.append(0)\n",
        "  news_author['true'] = true\n",
        "  news_author['mostly_true'] = mostly_true\n",
        "  news_author['half_true'] = half_true\n",
        "  news_author['barely_true'] = barely_true\n",
        "  news_author['false'] = false\n",
        "  news_author['pants_on_fire'] = pants_fire\n",
        "  news_author['total_true'] = news_author['true'] + news_author['mostly_true'] + news_author['half_true']\n",
        "  news_author['total_false'] = news_author['false'] + news_author['barely_true'] + news_author['pants_on_fire']\n",
        "  news_author['total_news'] = total_news\n",
        "  news_author['credibility'] = credibility\n",
        "  return news_author"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqDxKZx3wZlO"
      },
      "source": [
        "def generate_source_data(data):\n",
        "\n",
        "  true_count = 0\n",
        "  mostly_true_count =0\n",
        "  half_true_count=0\n",
        "  barely_true_count=0\n",
        "  mostly_false_count=0\n",
        "  false_count=0\n",
        "  pants_fire_count = 0\n",
        "  total_count=0\n",
        "\n",
        "  sources = ['Nur Ibrahim', 'Jordan Liles', 'Bethania Palma', 'Jessica Lee',\n",
        "        'Alex Kasprak', 'Dan MacGuill', 'Madison Dapcevich', 'Dan Evon',\n",
        "        'David Mikkelson']\n",
        "  true_list = []\n",
        "  mostly_true_list = []\n",
        "  half_true_list = []\n",
        "  barely_true_list = []\n",
        "  mostly_false_list = []\n",
        "  false_list = []\n",
        "  pants_fire_list = []\n",
        "  total_list = []\n",
        "  credibility_list = []\n",
        "\n",
        "  for src in sources:\n",
        "    new_rss_data = data[data[\"source\"] == src]\n",
        "    new_rss_data.reset_index(inplace = True)\n",
        "    for i in range(len(new_rss_data)):\n",
        "      if (new_rss_data['target'][i]=='true'):\n",
        "        true_count += 1\n",
        "      elif (new_rss_data['target'][i]=='mostly-true'):\n",
        "        mostly_true_count += 1\n",
        "      elif (new_rss_data['target'][i]=='half-true'):\n",
        "        half_true_count += 1\n",
        "      elif (new_rss_data['target'][i]=='barely-true'):\n",
        "         barely_true_count += 1\n",
        "      elif (new_rss_data['target'][i]=='mostly-false'):\n",
        "        mostly_false_count += 1\n",
        "      elif (new_rss_data['target'][i]=='false'):\n",
        "        false_count += 1\n",
        "      elif (new_rss_data['target'][i]=='pants-fire'):\n",
        "        pants_fire_count += 1\n",
        "    total_count = true_count + mostly_true_count + half_true_count + barely_true_count + mostly_false_count +  false_count + pants_fire_count\n",
        "    true_list.append(true_count)\n",
        "    mostly_true_list.append(mostly_true_count)\n",
        "    half_true_list.append(half_true_count)\n",
        "    barely_true_list.append(barely_true_count)\n",
        "    mostly_false_list.append(mostly_false_count)\n",
        "    false_list.append(false_count)\n",
        "    pants_fire_list.append(pants_fire_count)\n",
        "    total_list.append(total_count)\n",
        "    if (total_count > 0):\n",
        "      percent = [true_count/total_count, mostly_true_count/total_count, half_true_count/total_count, barely_true_count/total_count, \n",
        "                 mostly_false_count/total_count, false_count/total_count, pants_fire_count/total_count]\n",
        "      max_value = max(percent)\n",
        "      credibility_list.append(percent.index(max_value)+1)\n",
        "    else:\n",
        "      credibility_list.append(0)\n",
        "\n",
        "  #new data source\n",
        "  rss_data_new = pd.DataFrame(columns = ['source', 'true' ,'mostly_true', 'half_true', 'barely_true' , 'mostly_false', 'false', 'pants_on_fire','total_news', 'credibility']) \n",
        "  rss_data_new['source'] = sources\n",
        "  rss_data_new['true'] = true_list\n",
        "  rss_data_new['mostly_true'] = mostly_true_list\n",
        "  rss_data_new['half_true'] = half_true_list\n",
        "  rss_data_new['barely_true'] = barely_true_list\n",
        "  rss_data_new['mostly_false'] = mostly_false_list\n",
        "  rss_data_new['false'] = false_list\n",
        "  rss_data_new['pants_on_fire'] = pants_fire_list\n",
        "  rss_data_new['total_news'] = total_list\n",
        "  rss_data_new['credibility'] = credibility_list\n",
        "\n",
        "  return rss_data_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uuWkEi-wbGi"
      },
      "source": [
        "def get_reputation_score(total_true, total_news): # return between 0 and 1, being 0 = Reputable,  1 = Non-reputable\n",
        "  try:\n",
        "      true_percent = abs(total_true/total_news) * 100\n",
        "  except ZeroDivisionError:\n",
        "      true_percent = 0\n",
        "  if (true_percent >= 50):\n",
        "    return 0\n",
        "  else:\n",
        "    return 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El95s9hkwcpN"
      },
      "source": [
        "def get_credibility_score(data):\n",
        "  credibility_score = []\n",
        "  cred_mean = data['credibility'].mean()\n",
        "  for index, row in data.iterrows():\n",
        "    if (row['credibility'] >= cred_mean):\n",
        "      credibility_score.append(0) # 0 = Credible,  1 = Non-credible\n",
        "    else:\n",
        "      credibility_score.append(1)\n",
        "  return credibility_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9rFUQ3XweVH"
      },
      "source": [
        "def simplify_venue_label(input_label):  \n",
        "  true_labels = ['news','interview','television','show', 'speech', 'reporters', 'debate', 'newsletter', 'press', \n",
        "               'CNN', 'ABC', 'CBS', 'video', 'conference', 'official', 'book']\n",
        "  false_labels = ['website', 'tweet', 'mail', 'e-mail', 'mailer', 'web', 'site', 'meme', 'comic', 'advertisement', 'ad', 'blog', 'flier', \n",
        "                'letter', 'social', 'tweets', 'internet','facebook', 'message', 'campaign', 'post', 'handout', 'leaflet' ]\n",
        "  words = input_label.lower().split(\" \")\n",
        "  for s in words:\n",
        "    if s in true_labels:\n",
        "      return 0\n",
        "    elif s in false_labels:\n",
        "      return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXvWp9Wwf9_"
      },
      "source": [
        "def compute_repuation(data):\n",
        "  reput_score = []\n",
        "  for index, row in data.iterrows():\n",
        "    rs = get_reputation_score(row['total_true'],row['total_news'])\n",
        "    reput_score.append(rs)\n",
        "\n",
        "  data['reputation_score'] = reput_score\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kncc7EFRwhiw"
      },
      "source": [
        "def compute_credibility(data):\n",
        "  data['credibility_score'] = get_credibility_score(data)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAzBwfs-witO"
      },
      "source": [
        "def compute_authenticity(data):\n",
        "  data['authenticity_score'] = data.apply(lambda row: simplify_venue_label(str(row['context'])), axis=1)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPjDDwxAwkH-"
      },
      "source": [
        "def predict_long_term_utility(data, model, f):\n",
        "  y_pred = []\n",
        "  if f == 1:\n",
        "    stream_data = compute_repuation(data)\n",
        "    test_features = stream_data[['total_true','total_false','total_news','reputation_score']]\n",
        "    y_pred = model.predict_proba(test_features.values)\n",
        "  elif f == 2:\n",
        "    stream_data = compute_credibility(data)\n",
        "    test_features = stream_data[['total_true','total_false','total_news','credibility_score']]\n",
        "    y_pred = model.predict_proba(test_features.values)\n",
        "  elif f == 3:\n",
        "    stream_data = compute_authenticity(data)\n",
        "    test_features = stream_data[['total_true','total_false','total_news','authenticity_score']]\n",
        "    y_pred = model.predict_proba(test_features.values)\n",
        "  return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDbnRo-hwlyG"
      },
      "source": [
        "def f_long_term_utility_ensembling(stream_data):\n",
        "\n",
        "  rss_data_author = stream_data #dataset_preprocessing(stream_data)\n",
        "  \n",
        "  news_author_amalgamated = read_csv_from_drive(\"https://drive.google.com/file/d/1-J33YfIT8nuJiwMPw4e0n7uOJn1Hdcxc/view?usp=sharing\")\n",
        "  index=np.random.choice(news_author_amalgamated['context'].nunique(), size=(rss_data_author.shape[0]))\n",
        "  context_list=[]\n",
        "  for i in index:\n",
        "    context_list.append(news_author_amalgamated['context'][i])\n",
        "\n",
        "  rss_data_author['context']=context_list\n",
        "  rss_data_source = generate_source_data(rss_data_author)\n",
        "  rss_test_data = combine(rss_data_author, rss_data_source)\n",
        "\n",
        "  m1_data = read_from_drive('https://drive.google.com/file/d/1-LPitQ6AbS0M-eq3RDr2s3LgD6847qsv/view?usp=sharing')\n",
        "  reputation_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "  m1_data = read_from_drive('https://drive.google.com/file/d/1-PY_qbeK3j9SvYAFXMzTl6_u900FYUgL/view?usp=sharing')\n",
        "  credibility_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "  m1_data = read_from_drive('https://drive.google.com/file/d/1-QHh4d4HAN8rIJfekjrfu4jlDb85e2Fj/view?usp=sharing')\n",
        "  authenticity_model = cPickle.loads(m1_data.getvalue())\n",
        "\n",
        "  mf_y_pred_1 = predict_long_term_utility(rss_test_data, reputation_model, 1)\n",
        "  mf_y_pred_2 = predict_long_term_utility(rss_test_data, credibility_model, 2)\n",
        "  mf_y_pred_3 = predict_long_term_utility(rss_test_data, authenticity_model, 3)\n",
        "\n",
        "  #Weighted Ensembling Of MicroFactor Models\n",
        "  final_pred = (mf_y_pred_1*0.5 + mf_y_pred_2*0.2 + mf_y_pred_3*0.3)\n",
        "  return final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P0u4CLJwnG2"
      },
      "source": [
        "## data divers ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUKQ2xYhwrXr"
      },
      "source": [
        "def data_divers_ensemble(data):\n",
        "  lta_pred = f_long_term_utility_ensembling(data)\n",
        "  ma_pred = ma_prediction(data)\n",
        "  cr_pred = f_credibility_and_relability_ensembling(data)\n",
        "  return lta_pred*0.2 + ma_pred*0.1 + cr_pred*0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCvhUitT5WH4"
      },
      "source": [
        "## Amalgam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmr0cgsH5ej4"
      },
      "source": [
        "### Yellow Factor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLmHNSWn5VoB"
      },
      "source": [
        "question_words = ['who','what','where','why','when','whose','whom','would','will','how','which','should','could']\n",
        "def is_question(text):\n",
        "    if \"?\" in text or text.startswith(('who','what','where','why','when','whose','whom','would','will','how','which','should','could','did','do')): return 1\n",
        "    else: return 0\n",
        "\n",
        "def is_exclamation(headline):\n",
        "    if \"!\" in headline: return 1\n",
        "    else: return 0\n",
        "\n",
        "def is_money_related(headline):\n",
        "    if \"$\" in headline: return 1\n",
        "    else: return 0\n",
        "\n",
        "def starts_with_num(text):\n",
        "    if text.startswith(('1','2','3','4','5','6','7','8','9')): return 1\n",
        "    else: return 0\n",
        "\n",
        "def yellowness_preprocessing(df_politifact):\n",
        "    df_politifact.Text = df_politifact.Text.apply(str)\n",
        "    df_politifact['flesch'] = df_politifact['Text'].apply(textstat.flesch_reading_ease)\n",
        "    ##Since the flesch reading score is now inversely proportional to the yellowness factor\n",
        "    ##subtracting its value from the highest - 121.22\n",
        "    df_politifact['flesch'] = 121.22 - df_politifact['flesch'] \n",
        "    df_politifact['text_lower'] = df_politifact['Text'].apply(lambda x: x.lower())\n",
        "    df_politifact['is_q'] = df_politifact['text_lower'].apply(is_question)\n",
        "    df_politifact = df_politifact.drop(columns='text_lower')\n",
        "    df_politifact['is_exclam'] = df_politifact['Text'].apply(is_exclamation)\n",
        "    df_politifact['is_money'] = df_politifact['Text'].apply(is_money_related)\n",
        "    df_politifact['starts_num'] = df_politifact['Text'].apply(starts_with_num)\n",
        "    df_politifact['num_words'] = df_politifact['Text'].apply(lambda x: len(x.split()))\n",
        "    df_politifact['All_Caps'] = df_politifact['Text'].apply(lambda x: sum(map(str.isupper, x.split())))\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    results = []\n",
        "    for headline in df_politifact['Text']:\n",
        "        pol_score = sia.polarity_scores(headline)\n",
        "        pol_score['headline'] = headline\n",
        "        results.append(pol_score)\n",
        "    df_politifact['Sentiment_Score'] = pd.DataFrame(results)['compound']\n",
        "\n",
        "    return df_politifact"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnBF9tJE5kvy"
      },
      "source": [
        "### Deception & Authenticity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X3QaSA55kLP"
      },
      "source": [
        "def deception_preprocessing(preprocess_df):\n",
        "  preprocess_df.Text = preprocess_df.Text.apply(str)\n",
        "  preprocess_df['lex_count'] = preprocess_df['Text'].apply(textstat.lexicon_count)\n",
        "  preprocess_df['num_sentences'] = preprocess_df['Text'].apply(textstat.sentence_count)\n",
        "  preprocess_df['num_syllables'] = preprocess_df['Text'].apply(textstat.syllable_count)\n",
        "  preprocess_df['flesch_kincaid_grade'] = preprocess_df['Text'].apply(textstat.flesch_kincaid_grade)\n",
        "  preprocess_df['gunning_fog'] = preprocess_df['Text'].apply(textstat.gunning_fog)\n",
        "  preprocess_df['avg_syllables'] = preprocess_df['num_syllables'] / preprocess_df['lex_count']\n",
        "  preprocess_df['avg_sentence_length'] = preprocess_df['lex_count'] / preprocess_df['num_sentences']\n",
        "  sia = SentimentIntensityAnalyzer()\n",
        "  results = []\n",
        "  for headline in preprocess_df['Text']:\n",
        "      pol_score = sia.polarity_scores(headline)\n",
        "      pol_score['headline'] = headline\n",
        "      results.append(pol_score)\n",
        "  preprocess_df['Sentiment_Score'] = pd.DataFrame(results)['compound']\n",
        "\n",
        "  return preprocess_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBs1CKiL5slI"
      },
      "source": [
        "### Style Factor Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFJoXzyY5sCt"
      },
      "source": [
        "def getStyleFactor(text):\n",
        "  data = {'Text': [text.iloc[0]]}\n",
        "  df_test = pd.DataFrame(data)\n",
        "  prob_yellow = getYellowFactor(df_test)\n",
        "  prob_deception = getDeceptionScore(df_test)\n",
        "  style_proba = prob_yellow[0][2] * 0.5 + prob_deception[0][1] * 0.5\n",
        "  return style_proba\n",
        "\n",
        "def getModel(file_id, model_filename):\n",
        "  downloaded = gdrive.CreateFile({'id': file_id})\n",
        "  downloaded.GetContentFile(model_filename)\n",
        "  pickle_filepath = '/content/{}'.format(model_filename)\n",
        "  return pickle.load(open(pickle_filepath, 'rb'))\n",
        "\n",
        "def getYellowFactor(df_test):\n",
        "  yellow_model = getModel('1rvd_6U42CHn0HYBhzKHteNtgYEVGcoG7', 'yellowModel_Retrained.sav')\n",
        "  df_yellow = yellowness_preprocessing(df_test)\n",
        "  df_yellow_new = df_yellow[['flesch','is_q','is_exclam','is_money','starts_num','All_Caps','num_words','Sentiment_Score']]\n",
        "  return yellow_model.predict_proba(df_yellow_new)\n",
        "\n",
        "def getDeceptionScore(df_test):\n",
        "  deception_model = getModel('1q2d4jL8PeH4y--4P0cqp4cfIc8wbndoz', 'deception_model.sav')\n",
        "  df_deception = deception_preprocessing(df_test)\n",
        "  df_deception_new = df_deception[['avg_sentence_length','avg_syllables','flesch_kincaid_grade','gunning_fog','Sentiment_Score']]\n",
        "  return deception_model.predict_proba(df_deception_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EHyXA9Zd-CM"
      },
      "source": [
        "# 3.0. Automated Inference Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnQh4rgCvSK4"
      },
      "source": [
        "## 3.1. Helper: Pick a random news item"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sspSi0vxeQ04"
      },
      "source": [
        "def get_random_news_items(num_items):\n",
        "  random_news_items = df_test_headlines.sample(n=num_items)\n",
        "  return random_news_items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ8H3Y9ceDhM"
      },
      "source": [
        "## 3.2. Helper:Print prediction of news item"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXPuqpUyfGoN"
      },
      "source": [
        "def print_prediction(reading):\n",
        "  if reading > 0.9:\n",
        "    print (\"This news headline is: Pants on Fire\")\n",
        "  elif reading > 0.7 and reading < 0.9:\n",
        "    print (\"This news headline is: Somewhat False\")\n",
        "  elif reading > 0.5 and reading < 0.7:\n",
        "    print (\"This news headline is: Mostly False\")\n",
        "  elif reading > 0.3 and reading < 0.5:\n",
        "    print (\"This news headline is: Half True\")\n",
        "  elif reading > 0.1 and reading < 0.3:\n",
        "    print (\"This news headline is: Mostly True\")\n",
        "  elif reading < 0.1:\n",
        "    print (\"True\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqk1SUgEg_5k"
      },
      "source": [
        "## 3.3. Helper: Run automated pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1-VQtbUo69_"
      },
      "source": [
        "def run_automated_inference_pipeline(num_items):\n",
        "  X_headline = 'preprocessed_statement_text'\n",
        "  X_body = 'preprocessed_body'\n",
        "  random_news_items = get_random_news_items(num_items)\n",
        "\n",
        "  i = 0\n",
        "  while ( i < num_items):\n",
        "    news = random_news_items.sample(1)\n",
        "    print('\\nRunning [false-o-meter] on - %s ' % (news['text']))\n",
        "    reading = get_false_o_meter_reading(news, news.index[0], X_headline, X_body)\n",
        "    print('[false-o-meter] reading is %f ' %(reading))\n",
        "    print_prediction(reading)\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvdmRo0hEX4N"
      },
      "source": [
        "!pip install NRCLex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRSFK3w0hUX7"
      },
      "source": [
        "## 3.4. Invoke automated pipeline on 20 random news items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elezXf39cBuv"
      },
      "source": [
        "from nrclex import NRCLex\n",
        "num_news = 20\n",
        "reading = run_automated_inference_pipeline(num_news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryxWHlDIwzXH"
      },
      "source": [
        "\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}